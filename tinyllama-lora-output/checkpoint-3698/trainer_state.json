{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3698,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005409791723018664,
      "grad_norm": 1.059017539024353,
      "learning_rate": 0.0002,
      "loss": 2.8446,
      "step": 1
    },
    {
      "epoch": 0.005409791723018664,
      "grad_norm": 0.8726606369018555,
      "learning_rate": 0.00019967532467532467,
      "loss": 2.1264,
      "step": 10
    },
    {
      "epoch": 0.010819583446037328,
      "grad_norm": 0.8073344230651855,
      "learning_rate": 0.00019931457431457433,
      "loss": 2.1083,
      "step": 20
    },
    {
      "epoch": 0.016229375169055992,
      "grad_norm": 1.0709134340286255,
      "learning_rate": 0.00019895382395382396,
      "loss": 2.0486,
      "step": 30
    },
    {
      "epoch": 0.021639166892074655,
      "grad_norm": 0.7338757514953613,
      "learning_rate": 0.00019859307359307358,
      "loss": 1.925,
      "step": 40
    },
    {
      "epoch": 0.027048958615093318,
      "grad_norm": 0.7970049977302551,
      "learning_rate": 0.00019823232323232324,
      "loss": 1.8423,
      "step": 50
    },
    {
      "epoch": 0.032458750338111984,
      "grad_norm": 0.6533471941947937,
      "learning_rate": 0.00019787157287157287,
      "loss": 1.924,
      "step": 60
    },
    {
      "epoch": 0.03786854206113065,
      "grad_norm": 0.6856894493103027,
      "learning_rate": 0.00019751082251082252,
      "loss": 1.9863,
      "step": 70
    },
    {
      "epoch": 0.04327833378414931,
      "grad_norm": 0.8277533650398254,
      "learning_rate": 0.00019715007215007215,
      "loss": 1.9326,
      "step": 80
    },
    {
      "epoch": 0.04868812550716797,
      "grad_norm": 0.7274510860443115,
      "learning_rate": 0.00019678932178932178,
      "loss": 1.8669,
      "step": 90
    },
    {
      "epoch": 0.054097917230186636,
      "grad_norm": 0.6724557876586914,
      "learning_rate": 0.00019642857142857144,
      "loss": 2.0425,
      "step": 100
    },
    {
      "epoch": 0.0595077089532053,
      "grad_norm": 0.9028497934341431,
      "learning_rate": 0.00019606782106782106,
      "loss": 1.9021,
      "step": 110
    },
    {
      "epoch": 0.06491750067622397,
      "grad_norm": 0.7980664372444153,
      "learning_rate": 0.0001957070707070707,
      "loss": 1.7812,
      "step": 120
    },
    {
      "epoch": 0.07032729239924262,
      "grad_norm": 0.6464447975158691,
      "learning_rate": 0.00019534632034632038,
      "loss": 1.9205,
      "step": 130
    },
    {
      "epoch": 0.0757370841222613,
      "grad_norm": 0.7904561161994934,
      "learning_rate": 0.00019498556998557,
      "loss": 1.8637,
      "step": 140
    },
    {
      "epoch": 0.08114687584527995,
      "grad_norm": 0.7551696300506592,
      "learning_rate": 0.00019462481962481963,
      "loss": 1.8788,
      "step": 150
    },
    {
      "epoch": 0.08655666756829862,
      "grad_norm": 0.9249818325042725,
      "learning_rate": 0.0001942640692640693,
      "loss": 1.9456,
      "step": 160
    },
    {
      "epoch": 0.09196645929131729,
      "grad_norm": 0.7251177430152893,
      "learning_rate": 0.00019390331890331892,
      "loss": 1.8224,
      "step": 170
    },
    {
      "epoch": 0.09737625101433595,
      "grad_norm": 0.6280574202537537,
      "learning_rate": 0.00019354256854256855,
      "loss": 1.8845,
      "step": 180
    },
    {
      "epoch": 0.10278604273735462,
      "grad_norm": 0.6253561973571777,
      "learning_rate": 0.0001931818181818182,
      "loss": 1.9367,
      "step": 190
    },
    {
      "epoch": 0.10819583446037327,
      "grad_norm": 0.8188689947128296,
      "learning_rate": 0.00019282106782106783,
      "loss": 1.8891,
      "step": 200
    },
    {
      "epoch": 0.11360562618339194,
      "grad_norm": 0.7594181299209595,
      "learning_rate": 0.00019246031746031748,
      "loss": 1.9048,
      "step": 210
    },
    {
      "epoch": 0.1190154179064106,
      "grad_norm": 0.7585452198982239,
      "learning_rate": 0.0001920995670995671,
      "loss": 1.9558,
      "step": 220
    },
    {
      "epoch": 0.12442520962942927,
      "grad_norm": 0.9446502923965454,
      "learning_rate": 0.00019173881673881674,
      "loss": 1.8942,
      "step": 230
    },
    {
      "epoch": 0.12983500135244794,
      "grad_norm": 0.6433151364326477,
      "learning_rate": 0.0001913780663780664,
      "loss": 1.8436,
      "step": 240
    },
    {
      "epoch": 0.1352447930754666,
      "grad_norm": 0.5592084527015686,
      "learning_rate": 0.00019101731601731603,
      "loss": 1.7625,
      "step": 250
    },
    {
      "epoch": 0.14065458479848525,
      "grad_norm": 0.4931503236293793,
      "learning_rate": 0.00019065656565656565,
      "loss": 1.9114,
      "step": 260
    },
    {
      "epoch": 0.14606437652150392,
      "grad_norm": 0.6958634853363037,
      "learning_rate": 0.0001902958152958153,
      "loss": 1.8762,
      "step": 270
    },
    {
      "epoch": 0.1514741682445226,
      "grad_norm": 0.6245899796485901,
      "learning_rate": 0.00018993506493506494,
      "loss": 1.99,
      "step": 280
    },
    {
      "epoch": 0.15688395996754126,
      "grad_norm": 0.5055649876594543,
      "learning_rate": 0.0001895743145743146,
      "loss": 1.8428,
      "step": 290
    },
    {
      "epoch": 0.1622937516905599,
      "grad_norm": 0.5293422937393188,
      "learning_rate": 0.00018921356421356422,
      "loss": 1.8448,
      "step": 300
    },
    {
      "epoch": 0.16770354341357857,
      "grad_norm": 0.6527729630470276,
      "learning_rate": 0.00018885281385281385,
      "loss": 1.7641,
      "step": 310
    },
    {
      "epoch": 0.17311333513659724,
      "grad_norm": 0.7917768955230713,
      "learning_rate": 0.0001884920634920635,
      "loss": 1.8259,
      "step": 320
    },
    {
      "epoch": 0.1785231268596159,
      "grad_norm": 0.6750956773757935,
      "learning_rate": 0.00018813131313131313,
      "loss": 1.808,
      "step": 330
    },
    {
      "epoch": 0.18393291858263458,
      "grad_norm": 0.7741411924362183,
      "learning_rate": 0.00018777056277056276,
      "loss": 1.8341,
      "step": 340
    },
    {
      "epoch": 0.18934271030565322,
      "grad_norm": 0.47366008162498474,
      "learning_rate": 0.00018740981240981242,
      "loss": 1.8742,
      "step": 350
    },
    {
      "epoch": 0.1947525020286719,
      "grad_norm": 0.6600525975227356,
      "learning_rate": 0.00018704906204906205,
      "loss": 1.9166,
      "step": 360
    },
    {
      "epoch": 0.20016229375169056,
      "grad_norm": 0.6588009595870972,
      "learning_rate": 0.0001866883116883117,
      "loss": 1.8421,
      "step": 370
    },
    {
      "epoch": 0.20557208547470923,
      "grad_norm": 0.565498411655426,
      "learning_rate": 0.00018632756132756133,
      "loss": 1.9606,
      "step": 380
    },
    {
      "epoch": 0.21098187719772787,
      "grad_norm": 0.547174870967865,
      "learning_rate": 0.00018596681096681096,
      "loss": 1.8344,
      "step": 390
    },
    {
      "epoch": 0.21639166892074654,
      "grad_norm": 0.5524263381958008,
      "learning_rate": 0.00018560606060606061,
      "loss": 1.863,
      "step": 400
    },
    {
      "epoch": 0.2218014606437652,
      "grad_norm": 0.4915259778499603,
      "learning_rate": 0.00018524531024531024,
      "loss": 1.9827,
      "step": 410
    },
    {
      "epoch": 0.22721125236678388,
      "grad_norm": 0.6815498471260071,
      "learning_rate": 0.00018488455988455987,
      "loss": 1.9488,
      "step": 420
    },
    {
      "epoch": 0.23262104408980255,
      "grad_norm": 0.6518100500106812,
      "learning_rate": 0.00018452380952380955,
      "loss": 1.8479,
      "step": 430
    },
    {
      "epoch": 0.2380308358128212,
      "grad_norm": 0.7227243185043335,
      "learning_rate": 0.00018416305916305918,
      "loss": 1.9218,
      "step": 440
    },
    {
      "epoch": 0.24344062753583987,
      "grad_norm": 0.5665829181671143,
      "learning_rate": 0.0001838023088023088,
      "loss": 1.8321,
      "step": 450
    },
    {
      "epoch": 0.24885041925885854,
      "grad_norm": 0.4892509877681732,
      "learning_rate": 0.00018344155844155847,
      "loss": 1.8928,
      "step": 460
    },
    {
      "epoch": 0.2542602109818772,
      "grad_norm": 0.6209413409233093,
      "learning_rate": 0.0001830808080808081,
      "loss": 1.9402,
      "step": 470
    },
    {
      "epoch": 0.2596700027048959,
      "grad_norm": 0.6155035495758057,
      "learning_rate": 0.00018272005772005772,
      "loss": 1.819,
      "step": 480
    },
    {
      "epoch": 0.2650797944279145,
      "grad_norm": 0.665149986743927,
      "learning_rate": 0.00018235930735930738,
      "loss": 1.8378,
      "step": 490
    },
    {
      "epoch": 0.2704895861509332,
      "grad_norm": 0.6167111396789551,
      "learning_rate": 0.000181998556998557,
      "loss": 1.791,
      "step": 500
    },
    {
      "epoch": 0.27589937787395186,
      "grad_norm": 0.7149457931518555,
      "learning_rate": 0.00018163780663780666,
      "loss": 2.0031,
      "step": 510
    },
    {
      "epoch": 0.2813091695969705,
      "grad_norm": 0.5903168320655823,
      "learning_rate": 0.0001812770562770563,
      "loss": 1.8848,
      "step": 520
    },
    {
      "epoch": 0.2867189613199892,
      "grad_norm": 0.7675890922546387,
      "learning_rate": 0.00018091630591630592,
      "loss": 1.8321,
      "step": 530
    },
    {
      "epoch": 0.29212875304300784,
      "grad_norm": 0.5425834655761719,
      "learning_rate": 0.00018055555555555557,
      "loss": 1.8171,
      "step": 540
    },
    {
      "epoch": 0.2975385447660265,
      "grad_norm": 0.8715850114822388,
      "learning_rate": 0.0001801948051948052,
      "loss": 1.8835,
      "step": 550
    },
    {
      "epoch": 0.3029483364890452,
      "grad_norm": 0.7486017942428589,
      "learning_rate": 0.00017983405483405483,
      "loss": 1.9191,
      "step": 560
    },
    {
      "epoch": 0.3083581282120638,
      "grad_norm": 0.5335214138031006,
      "learning_rate": 0.0001794733044733045,
      "loss": 1.8834,
      "step": 570
    },
    {
      "epoch": 0.3137679199350825,
      "grad_norm": 0.6896681785583496,
      "learning_rate": 0.00017911255411255412,
      "loss": 1.9157,
      "step": 580
    },
    {
      "epoch": 0.31917771165810116,
      "grad_norm": 0.6010924577713013,
      "learning_rate": 0.00017875180375180377,
      "loss": 1.8523,
      "step": 590
    },
    {
      "epoch": 0.3245875033811198,
      "grad_norm": 0.6503409147262573,
      "learning_rate": 0.0001783910533910534,
      "loss": 1.9184,
      "step": 600
    },
    {
      "epoch": 0.3299972951041385,
      "grad_norm": 0.8366488218307495,
      "learning_rate": 0.00017803030303030303,
      "loss": 1.8036,
      "step": 610
    },
    {
      "epoch": 0.33540708682715714,
      "grad_norm": 0.6672912240028381,
      "learning_rate": 0.00017766955266955268,
      "loss": 1.8294,
      "step": 620
    },
    {
      "epoch": 0.34081687855017584,
      "grad_norm": 0.5601389408111572,
      "learning_rate": 0.0001773088023088023,
      "loss": 1.8152,
      "step": 630
    },
    {
      "epoch": 0.3462266702731945,
      "grad_norm": 0.5954350829124451,
      "learning_rate": 0.00017694805194805194,
      "loss": 1.9053,
      "step": 640
    },
    {
      "epoch": 0.3516364619962131,
      "grad_norm": 0.6904880404472351,
      "learning_rate": 0.0001765873015873016,
      "loss": 1.8537,
      "step": 650
    },
    {
      "epoch": 0.3570462537192318,
      "grad_norm": 0.8146301507949829,
      "learning_rate": 0.00017622655122655122,
      "loss": 1.8262,
      "step": 660
    },
    {
      "epoch": 0.36245604544225046,
      "grad_norm": 0.5134557485580444,
      "learning_rate": 0.00017586580086580088,
      "loss": 1.8367,
      "step": 670
    },
    {
      "epoch": 0.36786583716526916,
      "grad_norm": 0.616471529006958,
      "learning_rate": 0.0001755050505050505,
      "loss": 1.8668,
      "step": 680
    },
    {
      "epoch": 0.3732756288882878,
      "grad_norm": 0.6836187839508057,
      "learning_rate": 0.00017514430014430014,
      "loss": 1.8699,
      "step": 690
    },
    {
      "epoch": 0.37868542061130644,
      "grad_norm": 0.5652530193328857,
      "learning_rate": 0.0001747835497835498,
      "loss": 1.8916,
      "step": 700
    },
    {
      "epoch": 0.38409521233432514,
      "grad_norm": 0.6516938805580139,
      "learning_rate": 0.00017442279942279942,
      "loss": 1.7516,
      "step": 710
    },
    {
      "epoch": 0.3895050040573438,
      "grad_norm": 0.5531587600708008,
      "learning_rate": 0.00017406204906204905,
      "loss": 1.7882,
      "step": 720
    },
    {
      "epoch": 0.3949147957803625,
      "grad_norm": 0.7752224206924438,
      "learning_rate": 0.00017370129870129873,
      "loss": 1.8211,
      "step": 730
    },
    {
      "epoch": 0.4003245875033811,
      "grad_norm": 0.5578263401985168,
      "learning_rate": 0.00017334054834054836,
      "loss": 1.8538,
      "step": 740
    },
    {
      "epoch": 0.40573437922639977,
      "grad_norm": 0.6315100789070129,
      "learning_rate": 0.000172979797979798,
      "loss": 1.9853,
      "step": 750
    },
    {
      "epoch": 0.41114417094941846,
      "grad_norm": 0.43798404932022095,
      "learning_rate": 0.00017261904761904764,
      "loss": 1.8738,
      "step": 760
    },
    {
      "epoch": 0.4165539626724371,
      "grad_norm": 0.6304440498352051,
      "learning_rate": 0.00017225829725829727,
      "loss": 1.9469,
      "step": 770
    },
    {
      "epoch": 0.42196375439545575,
      "grad_norm": 0.7759708762168884,
      "learning_rate": 0.0001718975468975469,
      "loss": 1.9016,
      "step": 780
    },
    {
      "epoch": 0.42737354611847445,
      "grad_norm": 0.5285512804985046,
      "learning_rate": 0.00017153679653679656,
      "loss": 1.8198,
      "step": 790
    },
    {
      "epoch": 0.4327833378414931,
      "grad_norm": 0.5398285984992981,
      "learning_rate": 0.00017117604617604618,
      "loss": 1.9831,
      "step": 800
    },
    {
      "epoch": 0.4381931295645118,
      "grad_norm": 0.6068094968795776,
      "learning_rate": 0.00017081529581529584,
      "loss": 1.8616,
      "step": 810
    },
    {
      "epoch": 0.4436029212875304,
      "grad_norm": 0.7433792352676392,
      "learning_rate": 0.00017045454545454547,
      "loss": 1.851,
      "step": 820
    },
    {
      "epoch": 0.44901271301054907,
      "grad_norm": 0.6602022647857666,
      "learning_rate": 0.0001700937950937951,
      "loss": 1.8416,
      "step": 830
    },
    {
      "epoch": 0.45442250473356777,
      "grad_norm": 0.6631324887275696,
      "learning_rate": 0.00016973304473304475,
      "loss": 1.8185,
      "step": 840
    },
    {
      "epoch": 0.4598322964565864,
      "grad_norm": 0.6001608967781067,
      "learning_rate": 0.00016937229437229438,
      "loss": 1.9038,
      "step": 850
    },
    {
      "epoch": 0.4652420881796051,
      "grad_norm": 0.6499806046485901,
      "learning_rate": 0.000169011544011544,
      "loss": 1.917,
      "step": 860
    },
    {
      "epoch": 0.47065187990262375,
      "grad_norm": 0.586942195892334,
      "learning_rate": 0.00016865079365079366,
      "loss": 1.8851,
      "step": 870
    },
    {
      "epoch": 0.4760616716256424,
      "grad_norm": 0.7718134522438049,
      "learning_rate": 0.0001682900432900433,
      "loss": 1.8748,
      "step": 880
    },
    {
      "epoch": 0.4814714633486611,
      "grad_norm": 0.4805094301700592,
      "learning_rate": 0.00016792929292929295,
      "loss": 1.8942,
      "step": 890
    },
    {
      "epoch": 0.48688125507167973,
      "grad_norm": 0.5999564528465271,
      "learning_rate": 0.00016756854256854258,
      "loss": 1.9138,
      "step": 900
    },
    {
      "epoch": 0.49229104679469843,
      "grad_norm": 0.6692824959754944,
      "learning_rate": 0.0001672077922077922,
      "loss": 1.8909,
      "step": 910
    },
    {
      "epoch": 0.49770083851771707,
      "grad_norm": 0.5285724997520447,
      "learning_rate": 0.00016684704184704186,
      "loss": 1.7985,
      "step": 920
    },
    {
      "epoch": 0.5031106302407358,
      "grad_norm": 0.7078955173492432,
      "learning_rate": 0.0001664862914862915,
      "loss": 1.8239,
      "step": 930
    },
    {
      "epoch": 0.5085204219637544,
      "grad_norm": 0.6011053919792175,
      "learning_rate": 0.00016612554112554112,
      "loss": 1.9832,
      "step": 940
    },
    {
      "epoch": 0.513930213686773,
      "grad_norm": 0.5565224885940552,
      "learning_rate": 0.00016576479076479077,
      "loss": 1.864,
      "step": 950
    },
    {
      "epoch": 0.5193400054097917,
      "grad_norm": 0.5454429388046265,
      "learning_rate": 0.0001654040404040404,
      "loss": 1.8036,
      "step": 960
    },
    {
      "epoch": 0.5247497971328103,
      "grad_norm": 0.5949538350105286,
      "learning_rate": 0.00016504329004329006,
      "loss": 1.9102,
      "step": 970
    },
    {
      "epoch": 0.530159588855829,
      "grad_norm": 0.5487449765205383,
      "learning_rate": 0.00016468253968253969,
      "loss": 1.9203,
      "step": 980
    },
    {
      "epoch": 0.5355693805788477,
      "grad_norm": 0.6056748628616333,
      "learning_rate": 0.0001643217893217893,
      "loss": 1.913,
      "step": 990
    },
    {
      "epoch": 0.5409791723018664,
      "grad_norm": 0.5366542339324951,
      "learning_rate": 0.00016396103896103897,
      "loss": 2.0059,
      "step": 1000
    },
    {
      "epoch": 0.546388964024885,
      "grad_norm": 0.6302283406257629,
      "learning_rate": 0.0001636002886002886,
      "loss": 1.8036,
      "step": 1010
    },
    {
      "epoch": 0.5517987557479037,
      "grad_norm": 0.6164554357528687,
      "learning_rate": 0.00016323953823953823,
      "loss": 1.8036,
      "step": 1020
    },
    {
      "epoch": 0.5572085474709224,
      "grad_norm": 0.6789078116416931,
      "learning_rate": 0.0001628787878787879,
      "loss": 1.6945,
      "step": 1030
    },
    {
      "epoch": 0.562618339193941,
      "grad_norm": 0.48498860001564026,
      "learning_rate": 0.00016251803751803754,
      "loss": 1.7641,
      "step": 1040
    },
    {
      "epoch": 0.5680281309169597,
      "grad_norm": 0.6631343960762024,
      "learning_rate": 0.00016215728715728717,
      "loss": 1.8849,
      "step": 1050
    },
    {
      "epoch": 0.5734379226399784,
      "grad_norm": 0.7452103495597839,
      "learning_rate": 0.00016179653679653682,
      "loss": 1.7798,
      "step": 1060
    },
    {
      "epoch": 0.578847714362997,
      "grad_norm": 0.8816846609115601,
      "learning_rate": 0.00016143578643578645,
      "loss": 1.7529,
      "step": 1070
    },
    {
      "epoch": 0.5842575060860157,
      "grad_norm": 0.5046195387840271,
      "learning_rate": 0.00016107503607503608,
      "loss": 1.9397,
      "step": 1080
    },
    {
      "epoch": 0.5896672978090344,
      "grad_norm": 0.5664178133010864,
      "learning_rate": 0.00016071428571428573,
      "loss": 1.856,
      "step": 1090
    },
    {
      "epoch": 0.595077089532053,
      "grad_norm": 0.5056560635566711,
      "learning_rate": 0.00016035353535353536,
      "loss": 1.9021,
      "step": 1100
    },
    {
      "epoch": 0.6004868812550717,
      "grad_norm": 0.7600980401039124,
      "learning_rate": 0.00015999278499278502,
      "loss": 1.9386,
      "step": 1110
    },
    {
      "epoch": 0.6058966729780904,
      "grad_norm": 0.49010440707206726,
      "learning_rate": 0.00015963203463203465,
      "loss": 1.7763,
      "step": 1120
    },
    {
      "epoch": 0.611306464701109,
      "grad_norm": 0.639493465423584,
      "learning_rate": 0.00015927128427128427,
      "loss": 1.8212,
      "step": 1130
    },
    {
      "epoch": 0.6167162564241276,
      "grad_norm": 0.5328038334846497,
      "learning_rate": 0.00015891053391053393,
      "loss": 1.8048,
      "step": 1140
    },
    {
      "epoch": 0.6221260481471463,
      "grad_norm": 0.7070821523666382,
      "learning_rate": 0.00015854978354978356,
      "loss": 1.766,
      "step": 1150
    },
    {
      "epoch": 0.627535839870165,
      "grad_norm": 0.5350348353385925,
      "learning_rate": 0.00015818903318903319,
      "loss": 1.8915,
      "step": 1160
    },
    {
      "epoch": 0.6329456315931836,
      "grad_norm": 0.6215640306472778,
      "learning_rate": 0.00015782828282828284,
      "loss": 1.8864,
      "step": 1170
    },
    {
      "epoch": 0.6383554233162023,
      "grad_norm": 0.6567551493644714,
      "learning_rate": 0.00015746753246753247,
      "loss": 1.7831,
      "step": 1180
    },
    {
      "epoch": 0.643765215039221,
      "grad_norm": 0.6871814727783203,
      "learning_rate": 0.00015710678210678213,
      "loss": 1.7946,
      "step": 1190
    },
    {
      "epoch": 0.6491750067622396,
      "grad_norm": 0.6021092534065247,
      "learning_rate": 0.00015674603174603175,
      "loss": 1.8311,
      "step": 1200
    },
    {
      "epoch": 0.6545847984852583,
      "grad_norm": 0.7194110751152039,
      "learning_rate": 0.00015638528138528138,
      "loss": 1.9804,
      "step": 1210
    },
    {
      "epoch": 0.659994590208277,
      "grad_norm": 0.5840404033660889,
      "learning_rate": 0.00015602453102453104,
      "loss": 1.7647,
      "step": 1220
    },
    {
      "epoch": 0.6654043819312957,
      "grad_norm": 0.5988948345184326,
      "learning_rate": 0.00015566378066378067,
      "loss": 2.0096,
      "step": 1230
    },
    {
      "epoch": 0.6708141736543143,
      "grad_norm": 0.6294538378715515,
      "learning_rate": 0.0001553030303030303,
      "loss": 1.8635,
      "step": 1240
    },
    {
      "epoch": 0.676223965377333,
      "grad_norm": 0.5394187569618225,
      "learning_rate": 0.00015494227994227995,
      "loss": 1.8181,
      "step": 1250
    },
    {
      "epoch": 0.6816337571003517,
      "grad_norm": 0.7345010042190552,
      "learning_rate": 0.00015458152958152958,
      "loss": 1.8719,
      "step": 1260
    },
    {
      "epoch": 0.6870435488233703,
      "grad_norm": 0.6404237747192383,
      "learning_rate": 0.00015422077922077923,
      "loss": 1.7653,
      "step": 1270
    },
    {
      "epoch": 0.692453340546389,
      "grad_norm": 0.9784781336784363,
      "learning_rate": 0.00015386002886002886,
      "loss": 1.9163,
      "step": 1280
    },
    {
      "epoch": 0.6978631322694077,
      "grad_norm": 0.5598706007003784,
      "learning_rate": 0.0001534992784992785,
      "loss": 1.9082,
      "step": 1290
    },
    {
      "epoch": 0.7032729239924262,
      "grad_norm": 0.6366021633148193,
      "learning_rate": 0.00015313852813852815,
      "loss": 1.8769,
      "step": 1300
    },
    {
      "epoch": 0.7086827157154449,
      "grad_norm": 0.622099757194519,
      "learning_rate": 0.00015277777777777777,
      "loss": 1.7896,
      "step": 1310
    },
    {
      "epoch": 0.7140925074384636,
      "grad_norm": 0.6269367933273315,
      "learning_rate": 0.0001524170274170274,
      "loss": 1.9128,
      "step": 1320
    },
    {
      "epoch": 0.7195022991614822,
      "grad_norm": 0.5206162929534912,
      "learning_rate": 0.00015205627705627709,
      "loss": 1.7893,
      "step": 1330
    },
    {
      "epoch": 0.7249120908845009,
      "grad_norm": 0.7422391772270203,
      "learning_rate": 0.00015169552669552671,
      "loss": 1.8441,
      "step": 1340
    },
    {
      "epoch": 0.7303218826075196,
      "grad_norm": 0.7064006924629211,
      "learning_rate": 0.00015133477633477634,
      "loss": 1.7872,
      "step": 1350
    },
    {
      "epoch": 0.7357316743305383,
      "grad_norm": 0.6117797493934631,
      "learning_rate": 0.000150974025974026,
      "loss": 1.8979,
      "step": 1360
    },
    {
      "epoch": 0.7411414660535569,
      "grad_norm": 0.5858284831047058,
      "learning_rate": 0.00015061327561327563,
      "loss": 1.7749,
      "step": 1370
    },
    {
      "epoch": 0.7465512577765756,
      "grad_norm": 0.6121045351028442,
      "learning_rate": 0.00015025252525252526,
      "loss": 1.9224,
      "step": 1380
    },
    {
      "epoch": 0.7519610494995943,
      "grad_norm": 0.6619307398796082,
      "learning_rate": 0.0001498917748917749,
      "loss": 1.9187,
      "step": 1390
    },
    {
      "epoch": 0.7573708412226129,
      "grad_norm": 0.6060405969619751,
      "learning_rate": 0.00014953102453102454,
      "loss": 1.7415,
      "step": 1400
    },
    {
      "epoch": 0.7627806329456316,
      "grad_norm": 0.5514004230499268,
      "learning_rate": 0.0001491702741702742,
      "loss": 1.7773,
      "step": 1410
    },
    {
      "epoch": 0.7681904246686503,
      "grad_norm": 0.4593316316604614,
      "learning_rate": 0.00014880952380952382,
      "loss": 1.8858,
      "step": 1420
    },
    {
      "epoch": 0.7736002163916689,
      "grad_norm": 0.5705344676971436,
      "learning_rate": 0.00014844877344877345,
      "loss": 1.7774,
      "step": 1430
    },
    {
      "epoch": 0.7790100081146876,
      "grad_norm": 0.5963940024375916,
      "learning_rate": 0.0001480880230880231,
      "loss": 1.7429,
      "step": 1440
    },
    {
      "epoch": 0.7844197998377063,
      "grad_norm": 0.675260066986084,
      "learning_rate": 0.00014772727272727274,
      "loss": 1.7636,
      "step": 1450
    },
    {
      "epoch": 0.789829591560725,
      "grad_norm": 0.6816063523292542,
      "learning_rate": 0.00014736652236652236,
      "loss": 1.8685,
      "step": 1460
    },
    {
      "epoch": 0.7952393832837436,
      "grad_norm": 0.7371480464935303,
      "learning_rate": 0.00014700577200577202,
      "loss": 1.9245,
      "step": 1470
    },
    {
      "epoch": 0.8006491750067622,
      "grad_norm": 0.6947327256202698,
      "learning_rate": 0.00014664502164502165,
      "loss": 1.8086,
      "step": 1480
    },
    {
      "epoch": 0.806058966729781,
      "grad_norm": 0.7368499636650085,
      "learning_rate": 0.0001462842712842713,
      "loss": 1.864,
      "step": 1490
    },
    {
      "epoch": 0.8114687584527995,
      "grad_norm": 0.5610400438308716,
      "learning_rate": 0.00014592352092352093,
      "loss": 1.8851,
      "step": 1500
    },
    {
      "epoch": 0.8168785501758182,
      "grad_norm": 0.7834349870681763,
      "learning_rate": 0.00014556277056277056,
      "loss": 1.9015,
      "step": 1510
    },
    {
      "epoch": 0.8222883418988369,
      "grad_norm": 0.5669360756874084,
      "learning_rate": 0.00014520202020202022,
      "loss": 2.0224,
      "step": 1520
    },
    {
      "epoch": 0.8276981336218555,
      "grad_norm": 0.4870734214782715,
      "learning_rate": 0.00014484126984126984,
      "loss": 1.7564,
      "step": 1530
    },
    {
      "epoch": 0.8331079253448742,
      "grad_norm": 0.5225902199745178,
      "learning_rate": 0.00014448051948051947,
      "loss": 1.8781,
      "step": 1540
    },
    {
      "epoch": 0.8385177170678929,
      "grad_norm": 0.5214931964874268,
      "learning_rate": 0.00014411976911976913,
      "loss": 1.7205,
      "step": 1550
    },
    {
      "epoch": 0.8439275087909115,
      "grad_norm": 0.4475209712982178,
      "learning_rate": 0.00014375901875901876,
      "loss": 1.8891,
      "step": 1560
    },
    {
      "epoch": 0.8493373005139302,
      "grad_norm": 0.6716943383216858,
      "learning_rate": 0.0001433982683982684,
      "loss": 1.8253,
      "step": 1570
    },
    {
      "epoch": 0.8547470922369489,
      "grad_norm": 0.6378010511398315,
      "learning_rate": 0.00014303751803751804,
      "loss": 1.8487,
      "step": 1580
    },
    {
      "epoch": 0.8601568839599676,
      "grad_norm": 0.5030857920646667,
      "learning_rate": 0.00014267676767676767,
      "loss": 1.8493,
      "step": 1590
    },
    {
      "epoch": 0.8655666756829862,
      "grad_norm": 0.6044548153877258,
      "learning_rate": 0.00014231601731601732,
      "loss": 1.9812,
      "step": 1600
    },
    {
      "epoch": 0.8709764674060049,
      "grad_norm": 0.5744468569755554,
      "learning_rate": 0.00014195526695526695,
      "loss": 1.8597,
      "step": 1610
    },
    {
      "epoch": 0.8763862591290236,
      "grad_norm": 0.5757994651794434,
      "learning_rate": 0.00014159451659451658,
      "loss": 1.9718,
      "step": 1620
    },
    {
      "epoch": 0.8817960508520422,
      "grad_norm": 0.6140642762184143,
      "learning_rate": 0.00014123376623376626,
      "loss": 1.8345,
      "step": 1630
    },
    {
      "epoch": 0.8872058425750609,
      "grad_norm": 0.7643903493881226,
      "learning_rate": 0.0001408730158730159,
      "loss": 1.925,
      "step": 1640
    },
    {
      "epoch": 0.8926156342980796,
      "grad_norm": 0.6239079236984253,
      "learning_rate": 0.00014051226551226552,
      "loss": 1.9045,
      "step": 1650
    },
    {
      "epoch": 0.8980254260210981,
      "grad_norm": 0.5419132709503174,
      "learning_rate": 0.00014015151515151518,
      "loss": 1.9394,
      "step": 1660
    },
    {
      "epoch": 0.9034352177441168,
      "grad_norm": 0.8333095908164978,
      "learning_rate": 0.0001397907647907648,
      "loss": 1.8623,
      "step": 1670
    },
    {
      "epoch": 0.9088450094671355,
      "grad_norm": 0.6497560739517212,
      "learning_rate": 0.00013943001443001443,
      "loss": 1.792,
      "step": 1680
    },
    {
      "epoch": 0.9142548011901542,
      "grad_norm": 0.5532323718070984,
      "learning_rate": 0.0001390692640692641,
      "loss": 1.9688,
      "step": 1690
    },
    {
      "epoch": 0.9196645929131728,
      "grad_norm": 0.5008400678634644,
      "learning_rate": 0.00013870851370851372,
      "loss": 1.8395,
      "step": 1700
    },
    {
      "epoch": 0.9250743846361915,
      "grad_norm": 0.7492568492889404,
      "learning_rate": 0.00013834776334776337,
      "loss": 1.8202,
      "step": 1710
    },
    {
      "epoch": 0.9304841763592102,
      "grad_norm": 0.7254984974861145,
      "learning_rate": 0.000137987012987013,
      "loss": 1.7344,
      "step": 1720
    },
    {
      "epoch": 0.9358939680822288,
      "grad_norm": 0.6341206431388855,
      "learning_rate": 0.00013762626262626263,
      "loss": 1.8104,
      "step": 1730
    },
    {
      "epoch": 0.9413037598052475,
      "grad_norm": 0.7675759792327881,
      "learning_rate": 0.00013726551226551228,
      "loss": 1.8107,
      "step": 1740
    },
    {
      "epoch": 0.9467135515282662,
      "grad_norm": 0.553982675075531,
      "learning_rate": 0.0001369047619047619,
      "loss": 1.8293,
      "step": 1750
    },
    {
      "epoch": 0.9521233432512848,
      "grad_norm": 0.6352393627166748,
      "learning_rate": 0.00013654401154401154,
      "loss": 1.8023,
      "step": 1760
    },
    {
      "epoch": 0.9575331349743035,
      "grad_norm": 0.4945586919784546,
      "learning_rate": 0.0001361832611832612,
      "loss": 1.7561,
      "step": 1770
    },
    {
      "epoch": 0.9629429266973222,
      "grad_norm": 0.6724412441253662,
      "learning_rate": 0.00013582251082251083,
      "loss": 1.9073,
      "step": 1780
    },
    {
      "epoch": 0.9683527184203408,
      "grad_norm": 0.6759103536605835,
      "learning_rate": 0.00013546176046176048,
      "loss": 1.7535,
      "step": 1790
    },
    {
      "epoch": 0.9737625101433595,
      "grad_norm": 0.5044499039649963,
      "learning_rate": 0.0001351010101010101,
      "loss": 1.8399,
      "step": 1800
    },
    {
      "epoch": 0.9791723018663782,
      "grad_norm": 0.7834867835044861,
      "learning_rate": 0.00013474025974025974,
      "loss": 1.8751,
      "step": 1810
    },
    {
      "epoch": 0.9845820935893969,
      "grad_norm": 0.8018578886985779,
      "learning_rate": 0.0001343795093795094,
      "loss": 1.9355,
      "step": 1820
    },
    {
      "epoch": 0.9899918853124154,
      "grad_norm": 0.7870275974273682,
      "learning_rate": 0.00013401875901875902,
      "loss": 1.795,
      "step": 1830
    },
    {
      "epoch": 0.9954016770354341,
      "grad_norm": 0.7234494686126709,
      "learning_rate": 0.00013365800865800865,
      "loss": 1.7275,
      "step": 1840
    },
    {
      "epoch": 1.000540979172302,
      "grad_norm": 0.5748689770698547,
      "learning_rate": 0.0001332972582972583,
      "loss": 1.8068,
      "step": 1850
    },
    {
      "epoch": 1.0059507708953206,
      "grad_norm": 0.7719118595123291,
      "learning_rate": 0.00013293650793650793,
      "loss": 1.8258,
      "step": 1860
    },
    {
      "epoch": 1.0113605626183393,
      "grad_norm": 0.6300216913223267,
      "learning_rate": 0.00013257575757575756,
      "loss": 1.834,
      "step": 1870
    },
    {
      "epoch": 1.0167703543413578,
      "grad_norm": 0.4891371428966522,
      "learning_rate": 0.00013221500721500722,
      "loss": 1.8648,
      "step": 1880
    },
    {
      "epoch": 1.0221801460643765,
      "grad_norm": 0.5167306661605835,
      "learning_rate": 0.00013185425685425685,
      "loss": 1.8161,
      "step": 1890
    },
    {
      "epoch": 1.0275899377873952,
      "grad_norm": 0.7635621428489685,
      "learning_rate": 0.0001314935064935065,
      "loss": 2.0039,
      "step": 1900
    },
    {
      "epoch": 1.0329997295104139,
      "grad_norm": 0.6957008838653564,
      "learning_rate": 0.00013113275613275613,
      "loss": 1.826,
      "step": 1910
    },
    {
      "epoch": 1.0384095212334326,
      "grad_norm": 0.5777027010917664,
      "learning_rate": 0.00013077200577200576,
      "loss": 1.8405,
      "step": 1920
    },
    {
      "epoch": 1.0438193129564513,
      "grad_norm": 0.6674944162368774,
      "learning_rate": 0.00013041125541125544,
      "loss": 1.7293,
      "step": 1930
    },
    {
      "epoch": 1.04922910467947,
      "grad_norm": 0.5862687230110168,
      "learning_rate": 0.00013005050505050507,
      "loss": 1.6901,
      "step": 1940
    },
    {
      "epoch": 1.0546388964024884,
      "grad_norm": 0.6826134920120239,
      "learning_rate": 0.0001296897546897547,
      "loss": 1.8039,
      "step": 1950
    },
    {
      "epoch": 1.0600486881255071,
      "grad_norm": 0.625024676322937,
      "learning_rate": 0.00012932900432900435,
      "loss": 1.8206,
      "step": 1960
    },
    {
      "epoch": 1.0654584798485258,
      "grad_norm": 0.583039402961731,
      "learning_rate": 0.00012896825396825398,
      "loss": 1.7532,
      "step": 1970
    },
    {
      "epoch": 1.0708682715715445,
      "grad_norm": 0.7158045172691345,
      "learning_rate": 0.0001286075036075036,
      "loss": 1.8886,
      "step": 1980
    },
    {
      "epoch": 1.0762780632945632,
      "grad_norm": 0.6362215280532837,
      "learning_rate": 0.00012824675324675327,
      "loss": 1.7733,
      "step": 1990
    },
    {
      "epoch": 1.081687855017582,
      "grad_norm": 0.6422250866889954,
      "learning_rate": 0.0001278860028860029,
      "loss": 1.8428,
      "step": 2000
    },
    {
      "epoch": 1.0870976467406004,
      "grad_norm": 0.675713062286377,
      "learning_rate": 0.00012752525252525255,
      "loss": 1.8852,
      "step": 2010
    },
    {
      "epoch": 1.092507438463619,
      "grad_norm": 0.5344334840774536,
      "learning_rate": 0.00012716450216450218,
      "loss": 1.7831,
      "step": 2020
    },
    {
      "epoch": 1.0979172301866378,
      "grad_norm": 0.501966118812561,
      "learning_rate": 0.0001268037518037518,
      "loss": 1.7349,
      "step": 2030
    },
    {
      "epoch": 1.1033270219096565,
      "grad_norm": 0.5284408330917358,
      "learning_rate": 0.00012644300144300146,
      "loss": 1.9013,
      "step": 2040
    },
    {
      "epoch": 1.1087368136326752,
      "grad_norm": 0.6243382096290588,
      "learning_rate": 0.0001260822510822511,
      "loss": 1.771,
      "step": 2050
    },
    {
      "epoch": 1.1141466053556939,
      "grad_norm": 0.5007815957069397,
      "learning_rate": 0.00012572150072150072,
      "loss": 1.8608,
      "step": 2060
    },
    {
      "epoch": 1.1195563970787126,
      "grad_norm": 0.8057668805122375,
      "learning_rate": 0.00012536075036075037,
      "loss": 1.8285,
      "step": 2070
    },
    {
      "epoch": 1.124966188801731,
      "grad_norm": 0.6293959617614746,
      "learning_rate": 0.000125,
      "loss": 1.7841,
      "step": 2080
    },
    {
      "epoch": 1.1303759805247497,
      "grad_norm": 0.6715720891952515,
      "learning_rate": 0.00012463924963924963,
      "loss": 1.8301,
      "step": 2090
    },
    {
      "epoch": 1.1357857722477684,
      "grad_norm": 0.5990322232246399,
      "learning_rate": 0.0001242784992784993,
      "loss": 1.6874,
      "step": 2100
    },
    {
      "epoch": 1.1411955639707871,
      "grad_norm": 0.5380962491035461,
      "learning_rate": 0.00012391774891774891,
      "loss": 1.889,
      "step": 2110
    },
    {
      "epoch": 1.1466053556938058,
      "grad_norm": 0.6679108738899231,
      "learning_rate": 0.00012355699855699857,
      "loss": 1.8739,
      "step": 2120
    },
    {
      "epoch": 1.1520151474168245,
      "grad_norm": 0.5951972603797913,
      "learning_rate": 0.0001231962481962482,
      "loss": 1.7401,
      "step": 2130
    },
    {
      "epoch": 1.157424939139843,
      "grad_norm": 0.843855082988739,
      "learning_rate": 0.00012283549783549783,
      "loss": 1.8294,
      "step": 2140
    },
    {
      "epoch": 1.1628347308628617,
      "grad_norm": 0.6141040325164795,
      "learning_rate": 0.00012247474747474748,
      "loss": 1.7885,
      "step": 2150
    },
    {
      "epoch": 1.1682445225858804,
      "grad_norm": 0.6727426052093506,
      "learning_rate": 0.0001221139971139971,
      "loss": 1.9237,
      "step": 2160
    },
    {
      "epoch": 1.173654314308899,
      "grad_norm": 0.5863227248191833,
      "learning_rate": 0.00012175324675324675,
      "loss": 1.8598,
      "step": 2170
    },
    {
      "epoch": 1.1790641060319178,
      "grad_norm": 0.6633469462394714,
      "learning_rate": 0.0001213924963924964,
      "loss": 1.8077,
      "step": 2180
    },
    {
      "epoch": 1.1844738977549365,
      "grad_norm": 0.6858174204826355,
      "learning_rate": 0.00012103174603174602,
      "loss": 1.8313,
      "step": 2190
    },
    {
      "epoch": 1.1898836894779552,
      "grad_norm": 0.7209561467170715,
      "learning_rate": 0.00012067099567099567,
      "loss": 1.8075,
      "step": 2200
    },
    {
      "epoch": 1.1952934812009737,
      "grad_norm": 0.6770550608634949,
      "learning_rate": 0.00012031024531024531,
      "loss": 1.7947,
      "step": 2210
    },
    {
      "epoch": 1.2007032729239924,
      "grad_norm": 0.8124482035636902,
      "learning_rate": 0.00011994949494949495,
      "loss": 1.8837,
      "step": 2220
    },
    {
      "epoch": 1.206113064647011,
      "grad_norm": 0.7097203135490417,
      "learning_rate": 0.00011958874458874458,
      "loss": 1.8293,
      "step": 2230
    },
    {
      "epoch": 1.2115228563700298,
      "grad_norm": 0.6421441435813904,
      "learning_rate": 0.00011922799422799425,
      "loss": 1.6657,
      "step": 2240
    },
    {
      "epoch": 1.2169326480930485,
      "grad_norm": 0.5914726257324219,
      "learning_rate": 0.00011886724386724389,
      "loss": 1.8303,
      "step": 2250
    },
    {
      "epoch": 1.2223424398160672,
      "grad_norm": 0.8118573427200317,
      "learning_rate": 0.00011850649350649352,
      "loss": 1.8693,
      "step": 2260
    },
    {
      "epoch": 1.2277522315390859,
      "grad_norm": 0.6606744527816772,
      "learning_rate": 0.00011814574314574316,
      "loss": 1.8495,
      "step": 2270
    },
    {
      "epoch": 1.2331620232621043,
      "grad_norm": 0.5325820446014404,
      "learning_rate": 0.0001177849927849928,
      "loss": 1.9308,
      "step": 2280
    },
    {
      "epoch": 1.238571814985123,
      "grad_norm": 0.6149091124534607,
      "learning_rate": 0.00011742424242424244,
      "loss": 1.8422,
      "step": 2290
    },
    {
      "epoch": 1.2439816067081417,
      "grad_norm": 0.5730535387992859,
      "learning_rate": 0.00011706349206349207,
      "loss": 1.8236,
      "step": 2300
    },
    {
      "epoch": 1.2493913984311604,
      "grad_norm": 0.6014704704284668,
      "learning_rate": 0.00011670274170274171,
      "loss": 1.7745,
      "step": 2310
    },
    {
      "epoch": 1.2548011901541791,
      "grad_norm": 0.5372412204742432,
      "learning_rate": 0.00011634199134199136,
      "loss": 1.7932,
      "step": 2320
    },
    {
      "epoch": 1.2602109818771976,
      "grad_norm": 0.6948702335357666,
      "learning_rate": 0.00011598124098124098,
      "loss": 1.7315,
      "step": 2330
    },
    {
      "epoch": 1.2656207736002165,
      "grad_norm": 0.9167966842651367,
      "learning_rate": 0.00011562049062049063,
      "loss": 1.7945,
      "step": 2340
    },
    {
      "epoch": 1.271030565323235,
      "grad_norm": 0.5794386267662048,
      "learning_rate": 0.00011525974025974027,
      "loss": 1.8112,
      "step": 2350
    },
    {
      "epoch": 1.2764403570462537,
      "grad_norm": 0.5677459836006165,
      "learning_rate": 0.00011489898989898991,
      "loss": 1.8079,
      "step": 2360
    },
    {
      "epoch": 1.2818501487692724,
      "grad_norm": 0.6708372831344604,
      "learning_rate": 0.00011453823953823954,
      "loss": 1.8673,
      "step": 2370
    },
    {
      "epoch": 1.287259940492291,
      "grad_norm": 0.4653368890285492,
      "learning_rate": 0.00011417748917748918,
      "loss": 1.7149,
      "step": 2380
    },
    {
      "epoch": 1.2926697322153098,
      "grad_norm": 0.7396929860115051,
      "learning_rate": 0.00011381673881673882,
      "loss": 1.7728,
      "step": 2390
    },
    {
      "epoch": 1.2980795239383283,
      "grad_norm": 0.6196709275245667,
      "learning_rate": 0.00011345598845598846,
      "loss": 1.9051,
      "step": 2400
    },
    {
      "epoch": 1.303489315661347,
      "grad_norm": 0.7314475178718567,
      "learning_rate": 0.00011309523809523809,
      "loss": 1.7879,
      "step": 2410
    },
    {
      "epoch": 1.3088991073843657,
      "grad_norm": 0.6958074569702148,
      "learning_rate": 0.00011273448773448773,
      "loss": 1.7475,
      "step": 2420
    },
    {
      "epoch": 1.3143088991073844,
      "grad_norm": 0.6282969117164612,
      "learning_rate": 0.00011237373737373738,
      "loss": 1.8946,
      "step": 2430
    },
    {
      "epoch": 1.319718690830403,
      "grad_norm": 0.5970296859741211,
      "learning_rate": 0.00011201298701298702,
      "loss": 1.8048,
      "step": 2440
    },
    {
      "epoch": 1.3251284825534217,
      "grad_norm": 0.5691786408424377,
      "learning_rate": 0.00011165223665223665,
      "loss": 1.7047,
      "step": 2450
    },
    {
      "epoch": 1.3305382742764404,
      "grad_norm": 0.5828725695610046,
      "learning_rate": 0.00011129148629148629,
      "loss": 1.7571,
      "step": 2460
    },
    {
      "epoch": 1.335948065999459,
      "grad_norm": 0.7763209939002991,
      "learning_rate": 0.00011093073593073593,
      "loss": 1.8131,
      "step": 2470
    },
    {
      "epoch": 1.3413578577224776,
      "grad_norm": 0.5684813857078552,
      "learning_rate": 0.00011056998556998557,
      "loss": 1.858,
      "step": 2480
    },
    {
      "epoch": 1.3467676494454963,
      "grad_norm": 0.7212191224098206,
      "learning_rate": 0.0001102092352092352,
      "loss": 1.9711,
      "step": 2490
    },
    {
      "epoch": 1.352177441168515,
      "grad_norm": 0.6510563492774963,
      "learning_rate": 0.00010984848484848484,
      "loss": 1.7465,
      "step": 2500
    },
    {
      "epoch": 1.3575872328915337,
      "grad_norm": 0.5989896059036255,
      "learning_rate": 0.00010948773448773448,
      "loss": 1.6315,
      "step": 2510
    },
    {
      "epoch": 1.3629970246145524,
      "grad_norm": 0.5490729808807373,
      "learning_rate": 0.00010912698412698413,
      "loss": 1.818,
      "step": 2520
    },
    {
      "epoch": 1.368406816337571,
      "grad_norm": 0.6094787120819092,
      "learning_rate": 0.00010876623376623376,
      "loss": 1.7855,
      "step": 2530
    },
    {
      "epoch": 1.3738166080605896,
      "grad_norm": 0.5328908562660217,
      "learning_rate": 0.00010840548340548342,
      "loss": 1.7254,
      "step": 2540
    },
    {
      "epoch": 1.3792263997836083,
      "grad_norm": 0.7710531949996948,
      "learning_rate": 0.00010804473304473307,
      "loss": 1.7833,
      "step": 2550
    },
    {
      "epoch": 1.384636191506627,
      "grad_norm": 0.5582380890846252,
      "learning_rate": 0.0001076839826839827,
      "loss": 1.8529,
      "step": 2560
    },
    {
      "epoch": 1.3900459832296457,
      "grad_norm": 0.8822859525680542,
      "learning_rate": 0.00010732323232323234,
      "loss": 1.7939,
      "step": 2570
    },
    {
      "epoch": 1.3954557749526644,
      "grad_norm": 0.6184915900230408,
      "learning_rate": 0.00010696248196248198,
      "loss": 1.7734,
      "step": 2580
    },
    {
      "epoch": 1.4008655666756828,
      "grad_norm": 0.5608492493629456,
      "learning_rate": 0.00010660173160173161,
      "loss": 1.7491,
      "step": 2590
    },
    {
      "epoch": 1.4062753583987018,
      "grad_norm": 0.5640730261802673,
      "learning_rate": 0.00010624098124098125,
      "loss": 1.8128,
      "step": 2600
    },
    {
      "epoch": 1.4116851501217202,
      "grad_norm": 0.6378976106643677,
      "learning_rate": 0.00010588023088023089,
      "loss": 1.7751,
      "step": 2610
    },
    {
      "epoch": 1.417094941844739,
      "grad_norm": 0.6328241229057312,
      "learning_rate": 0.00010551948051948053,
      "loss": 1.736,
      "step": 2620
    },
    {
      "epoch": 1.4225047335677576,
      "grad_norm": 0.6812890768051147,
      "learning_rate": 0.00010515873015873016,
      "loss": 1.8422,
      "step": 2630
    },
    {
      "epoch": 1.4279145252907763,
      "grad_norm": 0.5100430846214294,
      "learning_rate": 0.0001047979797979798,
      "loss": 1.8522,
      "step": 2640
    },
    {
      "epoch": 1.433324317013795,
      "grad_norm": 0.691237211227417,
      "learning_rate": 0.00010443722943722945,
      "loss": 1.8587,
      "step": 2650
    },
    {
      "epoch": 1.4387341087368135,
      "grad_norm": 0.5038658380508423,
      "learning_rate": 0.00010407647907647909,
      "loss": 1.8409,
      "step": 2660
    },
    {
      "epoch": 1.4441439004598324,
      "grad_norm": 0.6618639230728149,
      "learning_rate": 0.00010371572871572872,
      "loss": 1.8407,
      "step": 2670
    },
    {
      "epoch": 1.449553692182851,
      "grad_norm": 0.5997622609138489,
      "learning_rate": 0.00010335497835497836,
      "loss": 1.7418,
      "step": 2680
    },
    {
      "epoch": 1.4549634839058696,
      "grad_norm": 0.495418518781662,
      "learning_rate": 0.000102994227994228,
      "loss": 1.78,
      "step": 2690
    },
    {
      "epoch": 1.4603732756288883,
      "grad_norm": 0.7518081068992615,
      "learning_rate": 0.00010263347763347764,
      "loss": 1.8881,
      "step": 2700
    },
    {
      "epoch": 1.465783067351907,
      "grad_norm": 0.600385844707489,
      "learning_rate": 0.00010227272727272727,
      "loss": 1.7336,
      "step": 2710
    },
    {
      "epoch": 1.4711928590749257,
      "grad_norm": 0.554000973701477,
      "learning_rate": 0.00010191197691197691,
      "loss": 1.8022,
      "step": 2720
    },
    {
      "epoch": 1.4766026507979442,
      "grad_norm": 0.7735956311225891,
      "learning_rate": 0.00010155122655122655,
      "loss": 1.7599,
      "step": 2730
    },
    {
      "epoch": 1.4820124425209629,
      "grad_norm": 0.587806761264801,
      "learning_rate": 0.0001011904761904762,
      "loss": 1.7545,
      "step": 2740
    },
    {
      "epoch": 1.4874222342439816,
      "grad_norm": 0.647062361240387,
      "learning_rate": 0.00010082972582972582,
      "loss": 1.7417,
      "step": 2750
    },
    {
      "epoch": 1.4928320259670003,
      "grad_norm": 0.6140472292900085,
      "learning_rate": 0.00010046897546897547,
      "loss": 1.8717,
      "step": 2760
    },
    {
      "epoch": 1.498241817690019,
      "grad_norm": 0.5784351825714111,
      "learning_rate": 0.00010010822510822511,
      "loss": 1.8528,
      "step": 2770
    },
    {
      "epoch": 1.5036516094130374,
      "grad_norm": 0.939140796661377,
      "learning_rate": 9.974747474747475e-05,
      "loss": 1.8616,
      "step": 2780
    },
    {
      "epoch": 1.5090614011360564,
      "grad_norm": 0.561713695526123,
      "learning_rate": 9.938672438672439e-05,
      "loss": 1.8314,
      "step": 2790
    },
    {
      "epoch": 1.5144711928590748,
      "grad_norm": 0.6377916932106018,
      "learning_rate": 9.902597402597403e-05,
      "loss": 1.6808,
      "step": 2800
    },
    {
      "epoch": 1.5198809845820938,
      "grad_norm": 0.5014219880104065,
      "learning_rate": 9.866522366522368e-05,
      "loss": 1.8483,
      "step": 2810
    },
    {
      "epoch": 1.5252907763051122,
      "grad_norm": 0.6416189074516296,
      "learning_rate": 9.83044733044733e-05,
      "loss": 1.7344,
      "step": 2820
    },
    {
      "epoch": 1.530700568028131,
      "grad_norm": 0.527767539024353,
      "learning_rate": 9.794372294372295e-05,
      "loss": 1.7351,
      "step": 2830
    },
    {
      "epoch": 1.5361103597511496,
      "grad_norm": 0.4503432810306549,
      "learning_rate": 9.758297258297259e-05,
      "loss": 1.7654,
      "step": 2840
    },
    {
      "epoch": 1.541520151474168,
      "grad_norm": 0.8823242783546448,
      "learning_rate": 9.722222222222223e-05,
      "loss": 1.7906,
      "step": 2850
    },
    {
      "epoch": 1.546929943197187,
      "grad_norm": 0.8000040650367737,
      "learning_rate": 9.686147186147186e-05,
      "loss": 1.8563,
      "step": 2860
    },
    {
      "epoch": 1.5523397349202055,
      "grad_norm": 0.8218516707420349,
      "learning_rate": 9.65007215007215e-05,
      "loss": 1.7826,
      "step": 2870
    },
    {
      "epoch": 1.5577495266432242,
      "grad_norm": 0.5908588767051697,
      "learning_rate": 9.613997113997114e-05,
      "loss": 1.7484,
      "step": 2880
    },
    {
      "epoch": 1.563159318366243,
      "grad_norm": 0.5984391570091248,
      "learning_rate": 9.577922077922078e-05,
      "loss": 1.6992,
      "step": 2890
    },
    {
      "epoch": 1.5685691100892616,
      "grad_norm": 0.5694805979728699,
      "learning_rate": 9.541847041847041e-05,
      "loss": 1.8712,
      "step": 2900
    },
    {
      "epoch": 1.5739789018122803,
      "grad_norm": 0.7169468402862549,
      "learning_rate": 9.505772005772007e-05,
      "loss": 1.969,
      "step": 2910
    },
    {
      "epoch": 1.5793886935352988,
      "grad_norm": 0.6731020212173462,
      "learning_rate": 9.469696969696971e-05,
      "loss": 1.8057,
      "step": 2920
    },
    {
      "epoch": 1.5847984852583177,
      "grad_norm": 0.6578049659729004,
      "learning_rate": 9.433621933621934e-05,
      "loss": 1.8464,
      "step": 2930
    },
    {
      "epoch": 1.5902082769813362,
      "grad_norm": 0.6680458188056946,
      "learning_rate": 9.397546897546898e-05,
      "loss": 1.7579,
      "step": 2940
    },
    {
      "epoch": 1.5956180687043549,
      "grad_norm": 0.6551367044448853,
      "learning_rate": 9.361471861471862e-05,
      "loss": 1.7882,
      "step": 2950
    },
    {
      "epoch": 1.6010278604273736,
      "grad_norm": 0.8830645084381104,
      "learning_rate": 9.325396825396826e-05,
      "loss": 1.8037,
      "step": 2960
    },
    {
      "epoch": 1.6064376521503922,
      "grad_norm": 0.6841064095497131,
      "learning_rate": 9.289321789321789e-05,
      "loss": 1.809,
      "step": 2970
    },
    {
      "epoch": 1.611847443873411,
      "grad_norm": 0.7406896352767944,
      "learning_rate": 9.253246753246754e-05,
      "loss": 1.7145,
      "step": 2980
    },
    {
      "epoch": 1.6172572355964294,
      "grad_norm": 0.7577963471412659,
      "learning_rate": 9.217171717171718e-05,
      "loss": 1.7533,
      "step": 2990
    },
    {
      "epoch": 1.6226670273194483,
      "grad_norm": 0.5707703232765198,
      "learning_rate": 9.181096681096682e-05,
      "loss": 1.848,
      "step": 3000
    },
    {
      "epoch": 1.6280768190424668,
      "grad_norm": 0.7727959156036377,
      "learning_rate": 9.145021645021645e-05,
      "loss": 1.837,
      "step": 3010
    },
    {
      "epoch": 1.6334866107654855,
      "grad_norm": 0.9029533863067627,
      "learning_rate": 9.108946608946609e-05,
      "loss": 1.8699,
      "step": 3020
    },
    {
      "epoch": 1.6388964024885042,
      "grad_norm": 0.686570942401886,
      "learning_rate": 9.072871572871573e-05,
      "loss": 1.7136,
      "step": 3030
    },
    {
      "epoch": 1.644306194211523,
      "grad_norm": 0.6069352030754089,
      "learning_rate": 9.036796536796537e-05,
      "loss": 1.8569,
      "step": 3040
    },
    {
      "epoch": 1.6497159859345416,
      "grad_norm": 0.658044695854187,
      "learning_rate": 9.0007215007215e-05,
      "loss": 1.7424,
      "step": 3050
    },
    {
      "epoch": 1.65512577765756,
      "grad_norm": 0.6367936134338379,
      "learning_rate": 8.964646464646466e-05,
      "loss": 1.7717,
      "step": 3060
    },
    {
      "epoch": 1.660535569380579,
      "grad_norm": 0.7849181294441223,
      "learning_rate": 8.92857142857143e-05,
      "loss": 1.8601,
      "step": 3070
    },
    {
      "epoch": 1.6659453611035975,
      "grad_norm": 0.5776723623275757,
      "learning_rate": 8.892496392496393e-05,
      "loss": 1.8814,
      "step": 3080
    },
    {
      "epoch": 1.6713551528266162,
      "grad_norm": 0.560427188873291,
      "learning_rate": 8.856421356421357e-05,
      "loss": 1.7318,
      "step": 3090
    },
    {
      "epoch": 1.6767649445496349,
      "grad_norm": 0.6509298086166382,
      "learning_rate": 8.820346320346321e-05,
      "loss": 1.7832,
      "step": 3100
    },
    {
      "epoch": 1.6821747362726533,
      "grad_norm": 0.7135399580001831,
      "learning_rate": 8.784271284271285e-05,
      "loss": 1.9348,
      "step": 3110
    },
    {
      "epoch": 1.6875845279956723,
      "grad_norm": 0.5975245237350464,
      "learning_rate": 8.748196248196248e-05,
      "loss": 1.8347,
      "step": 3120
    },
    {
      "epoch": 1.6929943197186907,
      "grad_norm": 0.6246079206466675,
      "learning_rate": 8.712121212121212e-05,
      "loss": 1.8552,
      "step": 3130
    },
    {
      "epoch": 1.6984041114417094,
      "grad_norm": 0.6166945099830627,
      "learning_rate": 8.676046176046177e-05,
      "loss": 1.7327,
      "step": 3140
    },
    {
      "epoch": 1.7038139031647281,
      "grad_norm": 0.5571082234382629,
      "learning_rate": 8.639971139971141e-05,
      "loss": 1.7909,
      "step": 3150
    },
    {
      "epoch": 1.7092236948877468,
      "grad_norm": 0.7227116823196411,
      "learning_rate": 8.603896103896104e-05,
      "loss": 1.7338,
      "step": 3160
    },
    {
      "epoch": 1.7146334866107655,
      "grad_norm": 0.5448179841041565,
      "learning_rate": 8.567821067821068e-05,
      "loss": 1.8182,
      "step": 3170
    },
    {
      "epoch": 1.720043278333784,
      "grad_norm": 0.5541513562202454,
      "learning_rate": 8.531746031746032e-05,
      "loss": 1.7549,
      "step": 3180
    },
    {
      "epoch": 1.725453070056803,
      "grad_norm": 0.6774485111236572,
      "learning_rate": 8.495670995670996e-05,
      "loss": 1.8686,
      "step": 3190
    },
    {
      "epoch": 1.7308628617798214,
      "grad_norm": 0.5630931854248047,
      "learning_rate": 8.459595959595959e-05,
      "loss": 1.7225,
      "step": 3200
    },
    {
      "epoch": 1.73627265350284,
      "grad_norm": 0.6754251718521118,
      "learning_rate": 8.423520923520925e-05,
      "loss": 1.7643,
      "step": 3210
    },
    {
      "epoch": 1.7416824452258588,
      "grad_norm": 0.47501692175865173,
      "learning_rate": 8.387445887445889e-05,
      "loss": 1.8024,
      "step": 3220
    },
    {
      "epoch": 1.7470922369488775,
      "grad_norm": 0.5571823120117188,
      "learning_rate": 8.351370851370852e-05,
      "loss": 1.9131,
      "step": 3230
    },
    {
      "epoch": 1.7525020286718962,
      "grad_norm": 0.5791082978248596,
      "learning_rate": 8.315295815295816e-05,
      "loss": 1.892,
      "step": 3240
    },
    {
      "epoch": 1.7579118203949147,
      "grad_norm": 0.5932286381721497,
      "learning_rate": 8.27922077922078e-05,
      "loss": 1.8743,
      "step": 3250
    },
    {
      "epoch": 1.7633216121179336,
      "grad_norm": 0.732574999332428,
      "learning_rate": 8.243145743145744e-05,
      "loss": 1.7996,
      "step": 3260
    },
    {
      "epoch": 1.768731403840952,
      "grad_norm": 0.7538799047470093,
      "learning_rate": 8.207070707070707e-05,
      "loss": 1.7749,
      "step": 3270
    },
    {
      "epoch": 1.7741411955639708,
      "grad_norm": 0.6947706937789917,
      "learning_rate": 8.170995670995671e-05,
      "loss": 1.8034,
      "step": 3280
    },
    {
      "epoch": 1.7795509872869895,
      "grad_norm": 0.5295731425285339,
      "learning_rate": 8.134920634920635e-05,
      "loss": 1.7385,
      "step": 3290
    },
    {
      "epoch": 1.7849607790100082,
      "grad_norm": 0.5981854796409607,
      "learning_rate": 8.0988455988456e-05,
      "loss": 1.8233,
      "step": 3300
    },
    {
      "epoch": 1.7903705707330269,
      "grad_norm": 0.685311496257782,
      "learning_rate": 8.062770562770562e-05,
      "loss": 1.8406,
      "step": 3310
    },
    {
      "epoch": 1.7957803624560453,
      "grad_norm": 0.7737095355987549,
      "learning_rate": 8.026695526695527e-05,
      "loss": 1.96,
      "step": 3320
    },
    {
      "epoch": 1.8011901541790643,
      "grad_norm": 0.8853969573974609,
      "learning_rate": 7.990620490620491e-05,
      "loss": 1.7718,
      "step": 3330
    },
    {
      "epoch": 1.8065999459020827,
      "grad_norm": 0.6886159181594849,
      "learning_rate": 7.954545454545455e-05,
      "loss": 1.9631,
      "step": 3340
    },
    {
      "epoch": 1.8120097376251014,
      "grad_norm": 0.5824671983718872,
      "learning_rate": 7.918470418470418e-05,
      "loss": 1.9432,
      "step": 3350
    },
    {
      "epoch": 1.8174195293481201,
      "grad_norm": 0.6573203802108765,
      "learning_rate": 7.882395382395382e-05,
      "loss": 1.7902,
      "step": 3360
    },
    {
      "epoch": 1.8228293210711386,
      "grad_norm": 0.6482689380645752,
      "learning_rate": 7.846320346320348e-05,
      "loss": 1.8927,
      "step": 3370
    },
    {
      "epoch": 1.8282391127941575,
      "grad_norm": 0.6681481599807739,
      "learning_rate": 7.81024531024531e-05,
      "loss": 1.847,
      "step": 3380
    },
    {
      "epoch": 1.833648904517176,
      "grad_norm": 0.6683822274208069,
      "learning_rate": 7.774170274170275e-05,
      "loss": 1.7883,
      "step": 3390
    },
    {
      "epoch": 1.839058696240195,
      "grad_norm": 0.6146317720413208,
      "learning_rate": 7.738095238095239e-05,
      "loss": 1.7835,
      "step": 3400
    },
    {
      "epoch": 1.8444684879632134,
      "grad_norm": 0.5073976516723633,
      "learning_rate": 7.702020202020203e-05,
      "loss": 1.8328,
      "step": 3410
    },
    {
      "epoch": 1.849878279686232,
      "grad_norm": 0.6550754308700562,
      "learning_rate": 7.665945165945166e-05,
      "loss": 1.9235,
      "step": 3420
    },
    {
      "epoch": 1.8552880714092508,
      "grad_norm": 0.5955948829650879,
      "learning_rate": 7.62987012987013e-05,
      "loss": 1.7205,
      "step": 3430
    },
    {
      "epoch": 1.8606978631322693,
      "grad_norm": 0.756870448589325,
      "learning_rate": 7.593795093795094e-05,
      "loss": 1.8814,
      "step": 3440
    },
    {
      "epoch": 1.8661076548552882,
      "grad_norm": 0.6271041035652161,
      "learning_rate": 7.557720057720059e-05,
      "loss": 1.7525,
      "step": 3450
    },
    {
      "epoch": 1.8715174465783067,
      "grad_norm": 0.772280752658844,
      "learning_rate": 7.521645021645021e-05,
      "loss": 1.85,
      "step": 3460
    },
    {
      "epoch": 1.8769272383013254,
      "grad_norm": 0.7514734268188477,
      "learning_rate": 7.485569985569986e-05,
      "loss": 1.9103,
      "step": 3470
    },
    {
      "epoch": 1.882337030024344,
      "grad_norm": 1.0703790187835693,
      "learning_rate": 7.44949494949495e-05,
      "loss": 1.7489,
      "step": 3480
    },
    {
      "epoch": 1.8877468217473627,
      "grad_norm": 0.6420645117759705,
      "learning_rate": 7.413419913419914e-05,
      "loss": 1.7915,
      "step": 3490
    },
    {
      "epoch": 1.8931566134703814,
      "grad_norm": 0.5953499674797058,
      "learning_rate": 7.377344877344877e-05,
      "loss": 1.9287,
      "step": 3500
    },
    {
      "epoch": 1.8985664051934,
      "grad_norm": 0.6354008316993713,
      "learning_rate": 7.341269841269841e-05,
      "loss": 1.8396,
      "step": 3510
    },
    {
      "epoch": 1.9039761969164188,
      "grad_norm": 0.721768319606781,
      "learning_rate": 7.305194805194807e-05,
      "loss": 1.8521,
      "step": 3520
    },
    {
      "epoch": 1.9093859886394373,
      "grad_norm": 1.023544430732727,
      "learning_rate": 7.26911976911977e-05,
      "loss": 1.8166,
      "step": 3530
    },
    {
      "epoch": 1.914795780362456,
      "grad_norm": 0.5791183114051819,
      "learning_rate": 7.233044733044734e-05,
      "loss": 1.8748,
      "step": 3540
    },
    {
      "epoch": 1.9202055720854747,
      "grad_norm": 0.7738423943519592,
      "learning_rate": 7.196969696969698e-05,
      "loss": 1.9068,
      "step": 3550
    },
    {
      "epoch": 1.9256153638084934,
      "grad_norm": 0.639610767364502,
      "learning_rate": 7.160894660894662e-05,
      "loss": 1.8105,
      "step": 3560
    },
    {
      "epoch": 1.931025155531512,
      "grad_norm": 0.47337818145751953,
      "learning_rate": 7.124819624819625e-05,
      "loss": 1.7744,
      "step": 3570
    },
    {
      "epoch": 1.9364349472545306,
      "grad_norm": 0.7621448636054993,
      "learning_rate": 7.088744588744589e-05,
      "loss": 1.7798,
      "step": 3580
    },
    {
      "epoch": 1.9418447389775495,
      "grad_norm": 0.614712119102478,
      "learning_rate": 7.052669552669553e-05,
      "loss": 1.761,
      "step": 3590
    },
    {
      "epoch": 1.947254530700568,
      "grad_norm": 0.5357441902160645,
      "learning_rate": 7.016594516594517e-05,
      "loss": 1.8719,
      "step": 3600
    },
    {
      "epoch": 1.9526643224235867,
      "grad_norm": 0.6692637801170349,
      "learning_rate": 6.98051948051948e-05,
      "loss": 1.7763,
      "step": 3610
    },
    {
      "epoch": 1.9580741141466054,
      "grad_norm": 0.49973616003990173,
      "learning_rate": 6.944444444444444e-05,
      "loss": 1.7547,
      "step": 3620
    },
    {
      "epoch": 1.963483905869624,
      "grad_norm": 0.6778360605239868,
      "learning_rate": 6.908369408369409e-05,
      "loss": 1.8783,
      "step": 3630
    },
    {
      "epoch": 1.9688936975926428,
      "grad_norm": 0.6741539239883423,
      "learning_rate": 6.872294372294373e-05,
      "loss": 1.661,
      "step": 3640
    },
    {
      "epoch": 1.9743034893156612,
      "grad_norm": 0.6905511021614075,
      "learning_rate": 6.836219336219336e-05,
      "loss": 1.9004,
      "step": 3650
    },
    {
      "epoch": 1.9797132810386802,
      "grad_norm": 0.6132948398590088,
      "learning_rate": 6.8001443001443e-05,
      "loss": 1.9334,
      "step": 3660
    },
    {
      "epoch": 1.9851230727616986,
      "grad_norm": 0.6923881769180298,
      "learning_rate": 6.764069264069265e-05,
      "loss": 1.6967,
      "step": 3670
    },
    {
      "epoch": 1.9905328644847173,
      "grad_norm": 0.610485315322876,
      "learning_rate": 6.727994227994228e-05,
      "loss": 1.7567,
      "step": 3680
    },
    {
      "epoch": 1.995942656207736,
      "grad_norm": 0.6126591563224792,
      "learning_rate": 6.691919191919192e-05,
      "loss": 1.7971,
      "step": 3690
    }
  ],
  "logging_steps": 10,
  "max_steps": 5544,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.419786367585485e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
