{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9986475520692455,
  "eval_steps": 500,
  "global_step": 5544,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005409791723018664,
      "grad_norm": 1.059017539024353,
      "learning_rate": 0.0002,
      "loss": 2.8446,
      "step": 1
    },
    {
      "epoch": 0.005409791723018664,
      "grad_norm": 0.8726606369018555,
      "learning_rate": 0.00019967532467532467,
      "loss": 2.1264,
      "step": 10
    },
    {
      "epoch": 0.010819583446037328,
      "grad_norm": 0.8073344230651855,
      "learning_rate": 0.00019931457431457433,
      "loss": 2.1083,
      "step": 20
    },
    {
      "epoch": 0.016229375169055992,
      "grad_norm": 1.0709134340286255,
      "learning_rate": 0.00019895382395382396,
      "loss": 2.0486,
      "step": 30
    },
    {
      "epoch": 0.021639166892074655,
      "grad_norm": 0.7338757514953613,
      "learning_rate": 0.00019859307359307358,
      "loss": 1.925,
      "step": 40
    },
    {
      "epoch": 0.027048958615093318,
      "grad_norm": 0.7970049977302551,
      "learning_rate": 0.00019823232323232324,
      "loss": 1.8423,
      "step": 50
    },
    {
      "epoch": 0.032458750338111984,
      "grad_norm": 0.6533471941947937,
      "learning_rate": 0.00019787157287157287,
      "loss": 1.924,
      "step": 60
    },
    {
      "epoch": 0.03786854206113065,
      "grad_norm": 0.6856894493103027,
      "learning_rate": 0.00019751082251082252,
      "loss": 1.9863,
      "step": 70
    },
    {
      "epoch": 0.04327833378414931,
      "grad_norm": 0.8277533650398254,
      "learning_rate": 0.00019715007215007215,
      "loss": 1.9326,
      "step": 80
    },
    {
      "epoch": 0.04868812550716797,
      "grad_norm": 0.7274510860443115,
      "learning_rate": 0.00019678932178932178,
      "loss": 1.8669,
      "step": 90
    },
    {
      "epoch": 0.054097917230186636,
      "grad_norm": 0.6724557876586914,
      "learning_rate": 0.00019642857142857144,
      "loss": 2.0425,
      "step": 100
    },
    {
      "epoch": 0.0595077089532053,
      "grad_norm": 0.9028497934341431,
      "learning_rate": 0.00019606782106782106,
      "loss": 1.9021,
      "step": 110
    },
    {
      "epoch": 0.06491750067622397,
      "grad_norm": 0.7980664372444153,
      "learning_rate": 0.0001957070707070707,
      "loss": 1.7812,
      "step": 120
    },
    {
      "epoch": 0.07032729239924262,
      "grad_norm": 0.6464447975158691,
      "learning_rate": 0.00019534632034632038,
      "loss": 1.9205,
      "step": 130
    },
    {
      "epoch": 0.0757370841222613,
      "grad_norm": 0.7904561161994934,
      "learning_rate": 0.00019498556998557,
      "loss": 1.8637,
      "step": 140
    },
    {
      "epoch": 0.08114687584527995,
      "grad_norm": 0.7551696300506592,
      "learning_rate": 0.00019462481962481963,
      "loss": 1.8788,
      "step": 150
    },
    {
      "epoch": 0.08655666756829862,
      "grad_norm": 0.9249818325042725,
      "learning_rate": 0.0001942640692640693,
      "loss": 1.9456,
      "step": 160
    },
    {
      "epoch": 0.09196645929131729,
      "grad_norm": 0.7251177430152893,
      "learning_rate": 0.00019390331890331892,
      "loss": 1.8224,
      "step": 170
    },
    {
      "epoch": 0.09737625101433595,
      "grad_norm": 0.6280574202537537,
      "learning_rate": 0.00019354256854256855,
      "loss": 1.8845,
      "step": 180
    },
    {
      "epoch": 0.10278604273735462,
      "grad_norm": 0.6253561973571777,
      "learning_rate": 0.0001931818181818182,
      "loss": 1.9367,
      "step": 190
    },
    {
      "epoch": 0.10819583446037327,
      "grad_norm": 0.8188689947128296,
      "learning_rate": 0.00019282106782106783,
      "loss": 1.8891,
      "step": 200
    },
    {
      "epoch": 0.11360562618339194,
      "grad_norm": 0.7594181299209595,
      "learning_rate": 0.00019246031746031748,
      "loss": 1.9048,
      "step": 210
    },
    {
      "epoch": 0.1190154179064106,
      "grad_norm": 0.7585452198982239,
      "learning_rate": 0.0001920995670995671,
      "loss": 1.9558,
      "step": 220
    },
    {
      "epoch": 0.12442520962942927,
      "grad_norm": 0.9446502923965454,
      "learning_rate": 0.00019173881673881674,
      "loss": 1.8942,
      "step": 230
    },
    {
      "epoch": 0.12983500135244794,
      "grad_norm": 0.6433151364326477,
      "learning_rate": 0.0001913780663780664,
      "loss": 1.8436,
      "step": 240
    },
    {
      "epoch": 0.1352447930754666,
      "grad_norm": 0.5592084527015686,
      "learning_rate": 0.00019101731601731603,
      "loss": 1.7625,
      "step": 250
    },
    {
      "epoch": 0.14065458479848525,
      "grad_norm": 0.4931503236293793,
      "learning_rate": 0.00019065656565656565,
      "loss": 1.9114,
      "step": 260
    },
    {
      "epoch": 0.14606437652150392,
      "grad_norm": 0.6958634853363037,
      "learning_rate": 0.0001902958152958153,
      "loss": 1.8762,
      "step": 270
    },
    {
      "epoch": 0.1514741682445226,
      "grad_norm": 0.6245899796485901,
      "learning_rate": 0.00018993506493506494,
      "loss": 1.99,
      "step": 280
    },
    {
      "epoch": 0.15688395996754126,
      "grad_norm": 0.5055649876594543,
      "learning_rate": 0.0001895743145743146,
      "loss": 1.8428,
      "step": 290
    },
    {
      "epoch": 0.1622937516905599,
      "grad_norm": 0.5293422937393188,
      "learning_rate": 0.00018921356421356422,
      "loss": 1.8448,
      "step": 300
    },
    {
      "epoch": 0.16770354341357857,
      "grad_norm": 0.6527729630470276,
      "learning_rate": 0.00018885281385281385,
      "loss": 1.7641,
      "step": 310
    },
    {
      "epoch": 0.17311333513659724,
      "grad_norm": 0.7917768955230713,
      "learning_rate": 0.0001884920634920635,
      "loss": 1.8259,
      "step": 320
    },
    {
      "epoch": 0.1785231268596159,
      "grad_norm": 0.6750956773757935,
      "learning_rate": 0.00018813131313131313,
      "loss": 1.808,
      "step": 330
    },
    {
      "epoch": 0.18393291858263458,
      "grad_norm": 0.7741411924362183,
      "learning_rate": 0.00018777056277056276,
      "loss": 1.8341,
      "step": 340
    },
    {
      "epoch": 0.18934271030565322,
      "grad_norm": 0.47366008162498474,
      "learning_rate": 0.00018740981240981242,
      "loss": 1.8742,
      "step": 350
    },
    {
      "epoch": 0.1947525020286719,
      "grad_norm": 0.6600525975227356,
      "learning_rate": 0.00018704906204906205,
      "loss": 1.9166,
      "step": 360
    },
    {
      "epoch": 0.20016229375169056,
      "grad_norm": 0.6588009595870972,
      "learning_rate": 0.0001866883116883117,
      "loss": 1.8421,
      "step": 370
    },
    {
      "epoch": 0.20557208547470923,
      "grad_norm": 0.565498411655426,
      "learning_rate": 0.00018632756132756133,
      "loss": 1.9606,
      "step": 380
    },
    {
      "epoch": 0.21098187719772787,
      "grad_norm": 0.547174870967865,
      "learning_rate": 0.00018596681096681096,
      "loss": 1.8344,
      "step": 390
    },
    {
      "epoch": 0.21639166892074654,
      "grad_norm": 0.5524263381958008,
      "learning_rate": 0.00018560606060606061,
      "loss": 1.863,
      "step": 400
    },
    {
      "epoch": 0.2218014606437652,
      "grad_norm": 0.4915259778499603,
      "learning_rate": 0.00018524531024531024,
      "loss": 1.9827,
      "step": 410
    },
    {
      "epoch": 0.22721125236678388,
      "grad_norm": 0.6815498471260071,
      "learning_rate": 0.00018488455988455987,
      "loss": 1.9488,
      "step": 420
    },
    {
      "epoch": 0.23262104408980255,
      "grad_norm": 0.6518100500106812,
      "learning_rate": 0.00018452380952380955,
      "loss": 1.8479,
      "step": 430
    },
    {
      "epoch": 0.2380308358128212,
      "grad_norm": 0.7227243185043335,
      "learning_rate": 0.00018416305916305918,
      "loss": 1.9218,
      "step": 440
    },
    {
      "epoch": 0.24344062753583987,
      "grad_norm": 0.5665829181671143,
      "learning_rate": 0.0001838023088023088,
      "loss": 1.8321,
      "step": 450
    },
    {
      "epoch": 0.24885041925885854,
      "grad_norm": 0.4892509877681732,
      "learning_rate": 0.00018344155844155847,
      "loss": 1.8928,
      "step": 460
    },
    {
      "epoch": 0.2542602109818772,
      "grad_norm": 0.6209413409233093,
      "learning_rate": 0.0001830808080808081,
      "loss": 1.9402,
      "step": 470
    },
    {
      "epoch": 0.2596700027048959,
      "grad_norm": 0.6155035495758057,
      "learning_rate": 0.00018272005772005772,
      "loss": 1.819,
      "step": 480
    },
    {
      "epoch": 0.2650797944279145,
      "grad_norm": 0.665149986743927,
      "learning_rate": 0.00018235930735930738,
      "loss": 1.8378,
      "step": 490
    },
    {
      "epoch": 0.2704895861509332,
      "grad_norm": 0.6167111396789551,
      "learning_rate": 0.000181998556998557,
      "loss": 1.791,
      "step": 500
    },
    {
      "epoch": 0.27589937787395186,
      "grad_norm": 0.7149457931518555,
      "learning_rate": 0.00018163780663780666,
      "loss": 2.0031,
      "step": 510
    },
    {
      "epoch": 0.2813091695969705,
      "grad_norm": 0.5903168320655823,
      "learning_rate": 0.0001812770562770563,
      "loss": 1.8848,
      "step": 520
    },
    {
      "epoch": 0.2867189613199892,
      "grad_norm": 0.7675890922546387,
      "learning_rate": 0.00018091630591630592,
      "loss": 1.8321,
      "step": 530
    },
    {
      "epoch": 0.29212875304300784,
      "grad_norm": 0.5425834655761719,
      "learning_rate": 0.00018055555555555557,
      "loss": 1.8171,
      "step": 540
    },
    {
      "epoch": 0.2975385447660265,
      "grad_norm": 0.8715850114822388,
      "learning_rate": 0.0001801948051948052,
      "loss": 1.8835,
      "step": 550
    },
    {
      "epoch": 0.3029483364890452,
      "grad_norm": 0.7486017942428589,
      "learning_rate": 0.00017983405483405483,
      "loss": 1.9191,
      "step": 560
    },
    {
      "epoch": 0.3083581282120638,
      "grad_norm": 0.5335214138031006,
      "learning_rate": 0.0001794733044733045,
      "loss": 1.8834,
      "step": 570
    },
    {
      "epoch": 0.3137679199350825,
      "grad_norm": 0.6896681785583496,
      "learning_rate": 0.00017911255411255412,
      "loss": 1.9157,
      "step": 580
    },
    {
      "epoch": 0.31917771165810116,
      "grad_norm": 0.6010924577713013,
      "learning_rate": 0.00017875180375180377,
      "loss": 1.8523,
      "step": 590
    },
    {
      "epoch": 0.3245875033811198,
      "grad_norm": 0.6503409147262573,
      "learning_rate": 0.0001783910533910534,
      "loss": 1.9184,
      "step": 600
    },
    {
      "epoch": 0.3299972951041385,
      "grad_norm": 0.8366488218307495,
      "learning_rate": 0.00017803030303030303,
      "loss": 1.8036,
      "step": 610
    },
    {
      "epoch": 0.33540708682715714,
      "grad_norm": 0.6672912240028381,
      "learning_rate": 0.00017766955266955268,
      "loss": 1.8294,
      "step": 620
    },
    {
      "epoch": 0.34081687855017584,
      "grad_norm": 0.5601389408111572,
      "learning_rate": 0.0001773088023088023,
      "loss": 1.8152,
      "step": 630
    },
    {
      "epoch": 0.3462266702731945,
      "grad_norm": 0.5954350829124451,
      "learning_rate": 0.00017694805194805194,
      "loss": 1.9053,
      "step": 640
    },
    {
      "epoch": 0.3516364619962131,
      "grad_norm": 0.6904880404472351,
      "learning_rate": 0.0001765873015873016,
      "loss": 1.8537,
      "step": 650
    },
    {
      "epoch": 0.3570462537192318,
      "grad_norm": 0.8146301507949829,
      "learning_rate": 0.00017622655122655122,
      "loss": 1.8262,
      "step": 660
    },
    {
      "epoch": 0.36245604544225046,
      "grad_norm": 0.5134557485580444,
      "learning_rate": 0.00017586580086580088,
      "loss": 1.8367,
      "step": 670
    },
    {
      "epoch": 0.36786583716526916,
      "grad_norm": 0.616471529006958,
      "learning_rate": 0.0001755050505050505,
      "loss": 1.8668,
      "step": 680
    },
    {
      "epoch": 0.3732756288882878,
      "grad_norm": 0.6836187839508057,
      "learning_rate": 0.00017514430014430014,
      "loss": 1.8699,
      "step": 690
    },
    {
      "epoch": 0.37868542061130644,
      "grad_norm": 0.5652530193328857,
      "learning_rate": 0.0001747835497835498,
      "loss": 1.8916,
      "step": 700
    },
    {
      "epoch": 0.38409521233432514,
      "grad_norm": 0.6516938805580139,
      "learning_rate": 0.00017442279942279942,
      "loss": 1.7516,
      "step": 710
    },
    {
      "epoch": 0.3895050040573438,
      "grad_norm": 0.5531587600708008,
      "learning_rate": 0.00017406204906204905,
      "loss": 1.7882,
      "step": 720
    },
    {
      "epoch": 0.3949147957803625,
      "grad_norm": 0.7752224206924438,
      "learning_rate": 0.00017370129870129873,
      "loss": 1.8211,
      "step": 730
    },
    {
      "epoch": 0.4003245875033811,
      "grad_norm": 0.5578263401985168,
      "learning_rate": 0.00017334054834054836,
      "loss": 1.8538,
      "step": 740
    },
    {
      "epoch": 0.40573437922639977,
      "grad_norm": 0.6315100789070129,
      "learning_rate": 0.000172979797979798,
      "loss": 1.9853,
      "step": 750
    },
    {
      "epoch": 0.41114417094941846,
      "grad_norm": 0.43798404932022095,
      "learning_rate": 0.00017261904761904764,
      "loss": 1.8738,
      "step": 760
    },
    {
      "epoch": 0.4165539626724371,
      "grad_norm": 0.6304440498352051,
      "learning_rate": 0.00017225829725829727,
      "loss": 1.9469,
      "step": 770
    },
    {
      "epoch": 0.42196375439545575,
      "grad_norm": 0.7759708762168884,
      "learning_rate": 0.0001718975468975469,
      "loss": 1.9016,
      "step": 780
    },
    {
      "epoch": 0.42737354611847445,
      "grad_norm": 0.5285512804985046,
      "learning_rate": 0.00017153679653679656,
      "loss": 1.8198,
      "step": 790
    },
    {
      "epoch": 0.4327833378414931,
      "grad_norm": 0.5398285984992981,
      "learning_rate": 0.00017117604617604618,
      "loss": 1.9831,
      "step": 800
    },
    {
      "epoch": 0.4381931295645118,
      "grad_norm": 0.6068094968795776,
      "learning_rate": 0.00017081529581529584,
      "loss": 1.8616,
      "step": 810
    },
    {
      "epoch": 0.4436029212875304,
      "grad_norm": 0.7433792352676392,
      "learning_rate": 0.00017045454545454547,
      "loss": 1.851,
      "step": 820
    },
    {
      "epoch": 0.44901271301054907,
      "grad_norm": 0.6602022647857666,
      "learning_rate": 0.0001700937950937951,
      "loss": 1.8416,
      "step": 830
    },
    {
      "epoch": 0.45442250473356777,
      "grad_norm": 0.6631324887275696,
      "learning_rate": 0.00016973304473304475,
      "loss": 1.8185,
      "step": 840
    },
    {
      "epoch": 0.4598322964565864,
      "grad_norm": 0.6001608967781067,
      "learning_rate": 0.00016937229437229438,
      "loss": 1.9038,
      "step": 850
    },
    {
      "epoch": 0.4652420881796051,
      "grad_norm": 0.6499806046485901,
      "learning_rate": 0.000169011544011544,
      "loss": 1.917,
      "step": 860
    },
    {
      "epoch": 0.47065187990262375,
      "grad_norm": 0.586942195892334,
      "learning_rate": 0.00016865079365079366,
      "loss": 1.8851,
      "step": 870
    },
    {
      "epoch": 0.4760616716256424,
      "grad_norm": 0.7718134522438049,
      "learning_rate": 0.0001682900432900433,
      "loss": 1.8748,
      "step": 880
    },
    {
      "epoch": 0.4814714633486611,
      "grad_norm": 0.4805094301700592,
      "learning_rate": 0.00016792929292929295,
      "loss": 1.8942,
      "step": 890
    },
    {
      "epoch": 0.48688125507167973,
      "grad_norm": 0.5999564528465271,
      "learning_rate": 0.00016756854256854258,
      "loss": 1.9138,
      "step": 900
    },
    {
      "epoch": 0.49229104679469843,
      "grad_norm": 0.6692824959754944,
      "learning_rate": 0.0001672077922077922,
      "loss": 1.8909,
      "step": 910
    },
    {
      "epoch": 0.49770083851771707,
      "grad_norm": 0.5285724997520447,
      "learning_rate": 0.00016684704184704186,
      "loss": 1.7985,
      "step": 920
    },
    {
      "epoch": 0.5031106302407358,
      "grad_norm": 0.7078955173492432,
      "learning_rate": 0.0001664862914862915,
      "loss": 1.8239,
      "step": 930
    },
    {
      "epoch": 0.5085204219637544,
      "grad_norm": 0.6011053919792175,
      "learning_rate": 0.00016612554112554112,
      "loss": 1.9832,
      "step": 940
    },
    {
      "epoch": 0.513930213686773,
      "grad_norm": 0.5565224885940552,
      "learning_rate": 0.00016576479076479077,
      "loss": 1.864,
      "step": 950
    },
    {
      "epoch": 0.5193400054097917,
      "grad_norm": 0.5454429388046265,
      "learning_rate": 0.0001654040404040404,
      "loss": 1.8036,
      "step": 960
    },
    {
      "epoch": 0.5247497971328103,
      "grad_norm": 0.5949538350105286,
      "learning_rate": 0.00016504329004329006,
      "loss": 1.9102,
      "step": 970
    },
    {
      "epoch": 0.530159588855829,
      "grad_norm": 0.5487449765205383,
      "learning_rate": 0.00016468253968253969,
      "loss": 1.9203,
      "step": 980
    },
    {
      "epoch": 0.5355693805788477,
      "grad_norm": 0.6056748628616333,
      "learning_rate": 0.0001643217893217893,
      "loss": 1.913,
      "step": 990
    },
    {
      "epoch": 0.5409791723018664,
      "grad_norm": 0.5366542339324951,
      "learning_rate": 0.00016396103896103897,
      "loss": 2.0059,
      "step": 1000
    },
    {
      "epoch": 0.546388964024885,
      "grad_norm": 0.6302283406257629,
      "learning_rate": 0.0001636002886002886,
      "loss": 1.8036,
      "step": 1010
    },
    {
      "epoch": 0.5517987557479037,
      "grad_norm": 0.6164554357528687,
      "learning_rate": 0.00016323953823953823,
      "loss": 1.8036,
      "step": 1020
    },
    {
      "epoch": 0.5572085474709224,
      "grad_norm": 0.6789078116416931,
      "learning_rate": 0.0001628787878787879,
      "loss": 1.6945,
      "step": 1030
    },
    {
      "epoch": 0.562618339193941,
      "grad_norm": 0.48498860001564026,
      "learning_rate": 0.00016251803751803754,
      "loss": 1.7641,
      "step": 1040
    },
    {
      "epoch": 0.5680281309169597,
      "grad_norm": 0.6631343960762024,
      "learning_rate": 0.00016215728715728717,
      "loss": 1.8849,
      "step": 1050
    },
    {
      "epoch": 0.5734379226399784,
      "grad_norm": 0.7452103495597839,
      "learning_rate": 0.00016179653679653682,
      "loss": 1.7798,
      "step": 1060
    },
    {
      "epoch": 0.578847714362997,
      "grad_norm": 0.8816846609115601,
      "learning_rate": 0.00016143578643578645,
      "loss": 1.7529,
      "step": 1070
    },
    {
      "epoch": 0.5842575060860157,
      "grad_norm": 0.5046195387840271,
      "learning_rate": 0.00016107503607503608,
      "loss": 1.9397,
      "step": 1080
    },
    {
      "epoch": 0.5896672978090344,
      "grad_norm": 0.5664178133010864,
      "learning_rate": 0.00016071428571428573,
      "loss": 1.856,
      "step": 1090
    },
    {
      "epoch": 0.595077089532053,
      "grad_norm": 0.5056560635566711,
      "learning_rate": 0.00016035353535353536,
      "loss": 1.9021,
      "step": 1100
    },
    {
      "epoch": 0.6004868812550717,
      "grad_norm": 0.7600980401039124,
      "learning_rate": 0.00015999278499278502,
      "loss": 1.9386,
      "step": 1110
    },
    {
      "epoch": 0.6058966729780904,
      "grad_norm": 0.49010440707206726,
      "learning_rate": 0.00015963203463203465,
      "loss": 1.7763,
      "step": 1120
    },
    {
      "epoch": 0.611306464701109,
      "grad_norm": 0.639493465423584,
      "learning_rate": 0.00015927128427128427,
      "loss": 1.8212,
      "step": 1130
    },
    {
      "epoch": 0.6167162564241276,
      "grad_norm": 0.5328038334846497,
      "learning_rate": 0.00015891053391053393,
      "loss": 1.8048,
      "step": 1140
    },
    {
      "epoch": 0.6221260481471463,
      "grad_norm": 0.7070821523666382,
      "learning_rate": 0.00015854978354978356,
      "loss": 1.766,
      "step": 1150
    },
    {
      "epoch": 0.627535839870165,
      "grad_norm": 0.5350348353385925,
      "learning_rate": 0.00015818903318903319,
      "loss": 1.8915,
      "step": 1160
    },
    {
      "epoch": 0.6329456315931836,
      "grad_norm": 0.6215640306472778,
      "learning_rate": 0.00015782828282828284,
      "loss": 1.8864,
      "step": 1170
    },
    {
      "epoch": 0.6383554233162023,
      "grad_norm": 0.6567551493644714,
      "learning_rate": 0.00015746753246753247,
      "loss": 1.7831,
      "step": 1180
    },
    {
      "epoch": 0.643765215039221,
      "grad_norm": 0.6871814727783203,
      "learning_rate": 0.00015710678210678213,
      "loss": 1.7946,
      "step": 1190
    },
    {
      "epoch": 0.6491750067622396,
      "grad_norm": 0.6021092534065247,
      "learning_rate": 0.00015674603174603175,
      "loss": 1.8311,
      "step": 1200
    },
    {
      "epoch": 0.6545847984852583,
      "grad_norm": 0.7194110751152039,
      "learning_rate": 0.00015638528138528138,
      "loss": 1.9804,
      "step": 1210
    },
    {
      "epoch": 0.659994590208277,
      "grad_norm": 0.5840404033660889,
      "learning_rate": 0.00015602453102453104,
      "loss": 1.7647,
      "step": 1220
    },
    {
      "epoch": 0.6654043819312957,
      "grad_norm": 0.5988948345184326,
      "learning_rate": 0.00015566378066378067,
      "loss": 2.0096,
      "step": 1230
    },
    {
      "epoch": 0.6708141736543143,
      "grad_norm": 0.6294538378715515,
      "learning_rate": 0.0001553030303030303,
      "loss": 1.8635,
      "step": 1240
    },
    {
      "epoch": 0.676223965377333,
      "grad_norm": 0.5394187569618225,
      "learning_rate": 0.00015494227994227995,
      "loss": 1.8181,
      "step": 1250
    },
    {
      "epoch": 0.6816337571003517,
      "grad_norm": 0.7345010042190552,
      "learning_rate": 0.00015458152958152958,
      "loss": 1.8719,
      "step": 1260
    },
    {
      "epoch": 0.6870435488233703,
      "grad_norm": 0.6404237747192383,
      "learning_rate": 0.00015422077922077923,
      "loss": 1.7653,
      "step": 1270
    },
    {
      "epoch": 0.692453340546389,
      "grad_norm": 0.9784781336784363,
      "learning_rate": 0.00015386002886002886,
      "loss": 1.9163,
      "step": 1280
    },
    {
      "epoch": 0.6978631322694077,
      "grad_norm": 0.5598706007003784,
      "learning_rate": 0.0001534992784992785,
      "loss": 1.9082,
      "step": 1290
    },
    {
      "epoch": 0.7032729239924262,
      "grad_norm": 0.6366021633148193,
      "learning_rate": 0.00015313852813852815,
      "loss": 1.8769,
      "step": 1300
    },
    {
      "epoch": 0.7086827157154449,
      "grad_norm": 0.622099757194519,
      "learning_rate": 0.00015277777777777777,
      "loss": 1.7896,
      "step": 1310
    },
    {
      "epoch": 0.7140925074384636,
      "grad_norm": 0.6269367933273315,
      "learning_rate": 0.0001524170274170274,
      "loss": 1.9128,
      "step": 1320
    },
    {
      "epoch": 0.7195022991614822,
      "grad_norm": 0.5206162929534912,
      "learning_rate": 0.00015205627705627709,
      "loss": 1.7893,
      "step": 1330
    },
    {
      "epoch": 0.7249120908845009,
      "grad_norm": 0.7422391772270203,
      "learning_rate": 0.00015169552669552671,
      "loss": 1.8441,
      "step": 1340
    },
    {
      "epoch": 0.7303218826075196,
      "grad_norm": 0.7064006924629211,
      "learning_rate": 0.00015133477633477634,
      "loss": 1.7872,
      "step": 1350
    },
    {
      "epoch": 0.7357316743305383,
      "grad_norm": 0.6117797493934631,
      "learning_rate": 0.000150974025974026,
      "loss": 1.8979,
      "step": 1360
    },
    {
      "epoch": 0.7411414660535569,
      "grad_norm": 0.5858284831047058,
      "learning_rate": 0.00015061327561327563,
      "loss": 1.7749,
      "step": 1370
    },
    {
      "epoch": 0.7465512577765756,
      "grad_norm": 0.6121045351028442,
      "learning_rate": 0.00015025252525252526,
      "loss": 1.9224,
      "step": 1380
    },
    {
      "epoch": 0.7519610494995943,
      "grad_norm": 0.6619307398796082,
      "learning_rate": 0.0001498917748917749,
      "loss": 1.9187,
      "step": 1390
    },
    {
      "epoch": 0.7573708412226129,
      "grad_norm": 0.6060405969619751,
      "learning_rate": 0.00014953102453102454,
      "loss": 1.7415,
      "step": 1400
    },
    {
      "epoch": 0.7627806329456316,
      "grad_norm": 0.5514004230499268,
      "learning_rate": 0.0001491702741702742,
      "loss": 1.7773,
      "step": 1410
    },
    {
      "epoch": 0.7681904246686503,
      "grad_norm": 0.4593316316604614,
      "learning_rate": 0.00014880952380952382,
      "loss": 1.8858,
      "step": 1420
    },
    {
      "epoch": 0.7736002163916689,
      "grad_norm": 0.5705344676971436,
      "learning_rate": 0.00014844877344877345,
      "loss": 1.7774,
      "step": 1430
    },
    {
      "epoch": 0.7790100081146876,
      "grad_norm": 0.5963940024375916,
      "learning_rate": 0.0001480880230880231,
      "loss": 1.7429,
      "step": 1440
    },
    {
      "epoch": 0.7844197998377063,
      "grad_norm": 0.675260066986084,
      "learning_rate": 0.00014772727272727274,
      "loss": 1.7636,
      "step": 1450
    },
    {
      "epoch": 0.789829591560725,
      "grad_norm": 0.6816063523292542,
      "learning_rate": 0.00014736652236652236,
      "loss": 1.8685,
      "step": 1460
    },
    {
      "epoch": 0.7952393832837436,
      "grad_norm": 0.7371480464935303,
      "learning_rate": 0.00014700577200577202,
      "loss": 1.9245,
      "step": 1470
    },
    {
      "epoch": 0.8006491750067622,
      "grad_norm": 0.6947327256202698,
      "learning_rate": 0.00014664502164502165,
      "loss": 1.8086,
      "step": 1480
    },
    {
      "epoch": 0.806058966729781,
      "grad_norm": 0.7368499636650085,
      "learning_rate": 0.0001462842712842713,
      "loss": 1.864,
      "step": 1490
    },
    {
      "epoch": 0.8114687584527995,
      "grad_norm": 0.5610400438308716,
      "learning_rate": 0.00014592352092352093,
      "loss": 1.8851,
      "step": 1500
    },
    {
      "epoch": 0.8168785501758182,
      "grad_norm": 0.7834349870681763,
      "learning_rate": 0.00014556277056277056,
      "loss": 1.9015,
      "step": 1510
    },
    {
      "epoch": 0.8222883418988369,
      "grad_norm": 0.5669360756874084,
      "learning_rate": 0.00014520202020202022,
      "loss": 2.0224,
      "step": 1520
    },
    {
      "epoch": 0.8276981336218555,
      "grad_norm": 0.4870734214782715,
      "learning_rate": 0.00014484126984126984,
      "loss": 1.7564,
      "step": 1530
    },
    {
      "epoch": 0.8331079253448742,
      "grad_norm": 0.5225902199745178,
      "learning_rate": 0.00014448051948051947,
      "loss": 1.8781,
      "step": 1540
    },
    {
      "epoch": 0.8385177170678929,
      "grad_norm": 0.5214931964874268,
      "learning_rate": 0.00014411976911976913,
      "loss": 1.7205,
      "step": 1550
    },
    {
      "epoch": 0.8439275087909115,
      "grad_norm": 0.4475209712982178,
      "learning_rate": 0.00014375901875901876,
      "loss": 1.8891,
      "step": 1560
    },
    {
      "epoch": 0.8493373005139302,
      "grad_norm": 0.6716943383216858,
      "learning_rate": 0.0001433982683982684,
      "loss": 1.8253,
      "step": 1570
    },
    {
      "epoch": 0.8547470922369489,
      "grad_norm": 0.6378010511398315,
      "learning_rate": 0.00014303751803751804,
      "loss": 1.8487,
      "step": 1580
    },
    {
      "epoch": 0.8601568839599676,
      "grad_norm": 0.5030857920646667,
      "learning_rate": 0.00014267676767676767,
      "loss": 1.8493,
      "step": 1590
    },
    {
      "epoch": 0.8655666756829862,
      "grad_norm": 0.6044548153877258,
      "learning_rate": 0.00014231601731601732,
      "loss": 1.9812,
      "step": 1600
    },
    {
      "epoch": 0.8709764674060049,
      "grad_norm": 0.5744468569755554,
      "learning_rate": 0.00014195526695526695,
      "loss": 1.8597,
      "step": 1610
    },
    {
      "epoch": 0.8763862591290236,
      "grad_norm": 0.5757994651794434,
      "learning_rate": 0.00014159451659451658,
      "loss": 1.9718,
      "step": 1620
    },
    {
      "epoch": 0.8817960508520422,
      "grad_norm": 0.6140642762184143,
      "learning_rate": 0.00014123376623376626,
      "loss": 1.8345,
      "step": 1630
    },
    {
      "epoch": 0.8872058425750609,
      "grad_norm": 0.7643903493881226,
      "learning_rate": 0.0001408730158730159,
      "loss": 1.925,
      "step": 1640
    },
    {
      "epoch": 0.8926156342980796,
      "grad_norm": 0.6239079236984253,
      "learning_rate": 0.00014051226551226552,
      "loss": 1.9045,
      "step": 1650
    },
    {
      "epoch": 0.8980254260210981,
      "grad_norm": 0.5419132709503174,
      "learning_rate": 0.00014015151515151518,
      "loss": 1.9394,
      "step": 1660
    },
    {
      "epoch": 0.9034352177441168,
      "grad_norm": 0.8333095908164978,
      "learning_rate": 0.0001397907647907648,
      "loss": 1.8623,
      "step": 1670
    },
    {
      "epoch": 0.9088450094671355,
      "grad_norm": 0.6497560739517212,
      "learning_rate": 0.00013943001443001443,
      "loss": 1.792,
      "step": 1680
    },
    {
      "epoch": 0.9142548011901542,
      "grad_norm": 0.5532323718070984,
      "learning_rate": 0.0001390692640692641,
      "loss": 1.9688,
      "step": 1690
    },
    {
      "epoch": 0.9196645929131728,
      "grad_norm": 0.5008400678634644,
      "learning_rate": 0.00013870851370851372,
      "loss": 1.8395,
      "step": 1700
    },
    {
      "epoch": 0.9250743846361915,
      "grad_norm": 0.7492568492889404,
      "learning_rate": 0.00013834776334776337,
      "loss": 1.8202,
      "step": 1710
    },
    {
      "epoch": 0.9304841763592102,
      "grad_norm": 0.7254984974861145,
      "learning_rate": 0.000137987012987013,
      "loss": 1.7344,
      "step": 1720
    },
    {
      "epoch": 0.9358939680822288,
      "grad_norm": 0.6341206431388855,
      "learning_rate": 0.00013762626262626263,
      "loss": 1.8104,
      "step": 1730
    },
    {
      "epoch": 0.9413037598052475,
      "grad_norm": 0.7675759792327881,
      "learning_rate": 0.00013726551226551228,
      "loss": 1.8107,
      "step": 1740
    },
    {
      "epoch": 0.9467135515282662,
      "grad_norm": 0.553982675075531,
      "learning_rate": 0.0001369047619047619,
      "loss": 1.8293,
      "step": 1750
    },
    {
      "epoch": 0.9521233432512848,
      "grad_norm": 0.6352393627166748,
      "learning_rate": 0.00013654401154401154,
      "loss": 1.8023,
      "step": 1760
    },
    {
      "epoch": 0.9575331349743035,
      "grad_norm": 0.4945586919784546,
      "learning_rate": 0.0001361832611832612,
      "loss": 1.7561,
      "step": 1770
    },
    {
      "epoch": 0.9629429266973222,
      "grad_norm": 0.6724412441253662,
      "learning_rate": 0.00013582251082251083,
      "loss": 1.9073,
      "step": 1780
    },
    {
      "epoch": 0.9683527184203408,
      "grad_norm": 0.6759103536605835,
      "learning_rate": 0.00013546176046176048,
      "loss": 1.7535,
      "step": 1790
    },
    {
      "epoch": 0.9737625101433595,
      "grad_norm": 0.5044499039649963,
      "learning_rate": 0.0001351010101010101,
      "loss": 1.8399,
      "step": 1800
    },
    {
      "epoch": 0.9791723018663782,
      "grad_norm": 0.7834867835044861,
      "learning_rate": 0.00013474025974025974,
      "loss": 1.8751,
      "step": 1810
    },
    {
      "epoch": 0.9845820935893969,
      "grad_norm": 0.8018578886985779,
      "learning_rate": 0.0001343795093795094,
      "loss": 1.9355,
      "step": 1820
    },
    {
      "epoch": 0.9899918853124154,
      "grad_norm": 0.7870275974273682,
      "learning_rate": 0.00013401875901875902,
      "loss": 1.795,
      "step": 1830
    },
    {
      "epoch": 0.9954016770354341,
      "grad_norm": 0.7234494686126709,
      "learning_rate": 0.00013365800865800865,
      "loss": 1.7275,
      "step": 1840
    },
    {
      "epoch": 1.000540979172302,
      "grad_norm": 0.5748689770698547,
      "learning_rate": 0.0001332972582972583,
      "loss": 1.8068,
      "step": 1850
    },
    {
      "epoch": 1.0059507708953206,
      "grad_norm": 0.7719118595123291,
      "learning_rate": 0.00013293650793650793,
      "loss": 1.8258,
      "step": 1860
    },
    {
      "epoch": 1.0113605626183393,
      "grad_norm": 0.6300216913223267,
      "learning_rate": 0.00013257575757575756,
      "loss": 1.834,
      "step": 1870
    },
    {
      "epoch": 1.0167703543413578,
      "grad_norm": 0.4891371428966522,
      "learning_rate": 0.00013221500721500722,
      "loss": 1.8648,
      "step": 1880
    },
    {
      "epoch": 1.0221801460643765,
      "grad_norm": 0.5167306661605835,
      "learning_rate": 0.00013185425685425685,
      "loss": 1.8161,
      "step": 1890
    },
    {
      "epoch": 1.0275899377873952,
      "grad_norm": 0.7635621428489685,
      "learning_rate": 0.0001314935064935065,
      "loss": 2.0039,
      "step": 1900
    },
    {
      "epoch": 1.0329997295104139,
      "grad_norm": 0.6957008838653564,
      "learning_rate": 0.00013113275613275613,
      "loss": 1.826,
      "step": 1910
    },
    {
      "epoch": 1.0384095212334326,
      "grad_norm": 0.5777027010917664,
      "learning_rate": 0.00013077200577200576,
      "loss": 1.8405,
      "step": 1920
    },
    {
      "epoch": 1.0438193129564513,
      "grad_norm": 0.6674944162368774,
      "learning_rate": 0.00013041125541125544,
      "loss": 1.7293,
      "step": 1930
    },
    {
      "epoch": 1.04922910467947,
      "grad_norm": 0.5862687230110168,
      "learning_rate": 0.00013005050505050507,
      "loss": 1.6901,
      "step": 1940
    },
    {
      "epoch": 1.0546388964024884,
      "grad_norm": 0.6826134920120239,
      "learning_rate": 0.0001296897546897547,
      "loss": 1.8039,
      "step": 1950
    },
    {
      "epoch": 1.0600486881255071,
      "grad_norm": 0.625024676322937,
      "learning_rate": 0.00012932900432900435,
      "loss": 1.8206,
      "step": 1960
    },
    {
      "epoch": 1.0654584798485258,
      "grad_norm": 0.583039402961731,
      "learning_rate": 0.00012896825396825398,
      "loss": 1.7532,
      "step": 1970
    },
    {
      "epoch": 1.0708682715715445,
      "grad_norm": 0.7158045172691345,
      "learning_rate": 0.0001286075036075036,
      "loss": 1.8886,
      "step": 1980
    },
    {
      "epoch": 1.0762780632945632,
      "grad_norm": 0.6362215280532837,
      "learning_rate": 0.00012824675324675327,
      "loss": 1.7733,
      "step": 1990
    },
    {
      "epoch": 1.081687855017582,
      "grad_norm": 0.6422250866889954,
      "learning_rate": 0.0001278860028860029,
      "loss": 1.8428,
      "step": 2000
    },
    {
      "epoch": 1.0870976467406004,
      "grad_norm": 0.675713062286377,
      "learning_rate": 0.00012752525252525255,
      "loss": 1.8852,
      "step": 2010
    },
    {
      "epoch": 1.092507438463619,
      "grad_norm": 0.5344334840774536,
      "learning_rate": 0.00012716450216450218,
      "loss": 1.7831,
      "step": 2020
    },
    {
      "epoch": 1.0979172301866378,
      "grad_norm": 0.501966118812561,
      "learning_rate": 0.0001268037518037518,
      "loss": 1.7349,
      "step": 2030
    },
    {
      "epoch": 1.1033270219096565,
      "grad_norm": 0.5284408330917358,
      "learning_rate": 0.00012644300144300146,
      "loss": 1.9013,
      "step": 2040
    },
    {
      "epoch": 1.1087368136326752,
      "grad_norm": 0.6243382096290588,
      "learning_rate": 0.0001260822510822511,
      "loss": 1.771,
      "step": 2050
    },
    {
      "epoch": 1.1141466053556939,
      "grad_norm": 0.5007815957069397,
      "learning_rate": 0.00012572150072150072,
      "loss": 1.8608,
      "step": 2060
    },
    {
      "epoch": 1.1195563970787126,
      "grad_norm": 0.8057668805122375,
      "learning_rate": 0.00012536075036075037,
      "loss": 1.8285,
      "step": 2070
    },
    {
      "epoch": 1.124966188801731,
      "grad_norm": 0.6293959617614746,
      "learning_rate": 0.000125,
      "loss": 1.7841,
      "step": 2080
    },
    {
      "epoch": 1.1303759805247497,
      "grad_norm": 0.6715720891952515,
      "learning_rate": 0.00012463924963924963,
      "loss": 1.8301,
      "step": 2090
    },
    {
      "epoch": 1.1357857722477684,
      "grad_norm": 0.5990322232246399,
      "learning_rate": 0.0001242784992784993,
      "loss": 1.6874,
      "step": 2100
    },
    {
      "epoch": 1.1411955639707871,
      "grad_norm": 0.5380962491035461,
      "learning_rate": 0.00012391774891774891,
      "loss": 1.889,
      "step": 2110
    },
    {
      "epoch": 1.1466053556938058,
      "grad_norm": 0.6679108738899231,
      "learning_rate": 0.00012355699855699857,
      "loss": 1.8739,
      "step": 2120
    },
    {
      "epoch": 1.1520151474168245,
      "grad_norm": 0.5951972603797913,
      "learning_rate": 0.0001231962481962482,
      "loss": 1.7401,
      "step": 2130
    },
    {
      "epoch": 1.157424939139843,
      "grad_norm": 0.843855082988739,
      "learning_rate": 0.00012283549783549783,
      "loss": 1.8294,
      "step": 2140
    },
    {
      "epoch": 1.1628347308628617,
      "grad_norm": 0.6141040325164795,
      "learning_rate": 0.00012247474747474748,
      "loss": 1.7885,
      "step": 2150
    },
    {
      "epoch": 1.1682445225858804,
      "grad_norm": 0.6727426052093506,
      "learning_rate": 0.0001221139971139971,
      "loss": 1.9237,
      "step": 2160
    },
    {
      "epoch": 1.173654314308899,
      "grad_norm": 0.5863227248191833,
      "learning_rate": 0.00012175324675324675,
      "loss": 1.8598,
      "step": 2170
    },
    {
      "epoch": 1.1790641060319178,
      "grad_norm": 0.6633469462394714,
      "learning_rate": 0.0001213924963924964,
      "loss": 1.8077,
      "step": 2180
    },
    {
      "epoch": 1.1844738977549365,
      "grad_norm": 0.6858174204826355,
      "learning_rate": 0.00012103174603174602,
      "loss": 1.8313,
      "step": 2190
    },
    {
      "epoch": 1.1898836894779552,
      "grad_norm": 0.7209561467170715,
      "learning_rate": 0.00012067099567099567,
      "loss": 1.8075,
      "step": 2200
    },
    {
      "epoch": 1.1952934812009737,
      "grad_norm": 0.6770550608634949,
      "learning_rate": 0.00012031024531024531,
      "loss": 1.7947,
      "step": 2210
    },
    {
      "epoch": 1.2007032729239924,
      "grad_norm": 0.8124482035636902,
      "learning_rate": 0.00011994949494949495,
      "loss": 1.8837,
      "step": 2220
    },
    {
      "epoch": 1.206113064647011,
      "grad_norm": 0.7097203135490417,
      "learning_rate": 0.00011958874458874458,
      "loss": 1.8293,
      "step": 2230
    },
    {
      "epoch": 1.2115228563700298,
      "grad_norm": 0.6421441435813904,
      "learning_rate": 0.00011922799422799425,
      "loss": 1.6657,
      "step": 2240
    },
    {
      "epoch": 1.2169326480930485,
      "grad_norm": 0.5914726257324219,
      "learning_rate": 0.00011886724386724389,
      "loss": 1.8303,
      "step": 2250
    },
    {
      "epoch": 1.2223424398160672,
      "grad_norm": 0.8118573427200317,
      "learning_rate": 0.00011850649350649352,
      "loss": 1.8693,
      "step": 2260
    },
    {
      "epoch": 1.2277522315390859,
      "grad_norm": 0.6606744527816772,
      "learning_rate": 0.00011814574314574316,
      "loss": 1.8495,
      "step": 2270
    },
    {
      "epoch": 1.2331620232621043,
      "grad_norm": 0.5325820446014404,
      "learning_rate": 0.0001177849927849928,
      "loss": 1.9308,
      "step": 2280
    },
    {
      "epoch": 1.238571814985123,
      "grad_norm": 0.6149091124534607,
      "learning_rate": 0.00011742424242424244,
      "loss": 1.8422,
      "step": 2290
    },
    {
      "epoch": 1.2439816067081417,
      "grad_norm": 0.5730535387992859,
      "learning_rate": 0.00011706349206349207,
      "loss": 1.8236,
      "step": 2300
    },
    {
      "epoch": 1.2493913984311604,
      "grad_norm": 0.6014704704284668,
      "learning_rate": 0.00011670274170274171,
      "loss": 1.7745,
      "step": 2310
    },
    {
      "epoch": 1.2548011901541791,
      "grad_norm": 0.5372412204742432,
      "learning_rate": 0.00011634199134199136,
      "loss": 1.7932,
      "step": 2320
    },
    {
      "epoch": 1.2602109818771976,
      "grad_norm": 0.6948702335357666,
      "learning_rate": 0.00011598124098124098,
      "loss": 1.7315,
      "step": 2330
    },
    {
      "epoch": 1.2656207736002165,
      "grad_norm": 0.9167966842651367,
      "learning_rate": 0.00011562049062049063,
      "loss": 1.7945,
      "step": 2340
    },
    {
      "epoch": 1.271030565323235,
      "grad_norm": 0.5794386267662048,
      "learning_rate": 0.00011525974025974027,
      "loss": 1.8112,
      "step": 2350
    },
    {
      "epoch": 1.2764403570462537,
      "grad_norm": 0.5677459836006165,
      "learning_rate": 0.00011489898989898991,
      "loss": 1.8079,
      "step": 2360
    },
    {
      "epoch": 1.2818501487692724,
      "grad_norm": 0.6708372831344604,
      "learning_rate": 0.00011453823953823954,
      "loss": 1.8673,
      "step": 2370
    },
    {
      "epoch": 1.287259940492291,
      "grad_norm": 0.4653368890285492,
      "learning_rate": 0.00011417748917748918,
      "loss": 1.7149,
      "step": 2380
    },
    {
      "epoch": 1.2926697322153098,
      "grad_norm": 0.7396929860115051,
      "learning_rate": 0.00011381673881673882,
      "loss": 1.7728,
      "step": 2390
    },
    {
      "epoch": 1.2980795239383283,
      "grad_norm": 0.6196709275245667,
      "learning_rate": 0.00011345598845598846,
      "loss": 1.9051,
      "step": 2400
    },
    {
      "epoch": 1.303489315661347,
      "grad_norm": 0.7314475178718567,
      "learning_rate": 0.00011309523809523809,
      "loss": 1.7879,
      "step": 2410
    },
    {
      "epoch": 1.3088991073843657,
      "grad_norm": 0.6958074569702148,
      "learning_rate": 0.00011273448773448773,
      "loss": 1.7475,
      "step": 2420
    },
    {
      "epoch": 1.3143088991073844,
      "grad_norm": 0.6282969117164612,
      "learning_rate": 0.00011237373737373738,
      "loss": 1.8946,
      "step": 2430
    },
    {
      "epoch": 1.319718690830403,
      "grad_norm": 0.5970296859741211,
      "learning_rate": 0.00011201298701298702,
      "loss": 1.8048,
      "step": 2440
    },
    {
      "epoch": 1.3251284825534217,
      "grad_norm": 0.5691786408424377,
      "learning_rate": 0.00011165223665223665,
      "loss": 1.7047,
      "step": 2450
    },
    {
      "epoch": 1.3305382742764404,
      "grad_norm": 0.5828725695610046,
      "learning_rate": 0.00011129148629148629,
      "loss": 1.7571,
      "step": 2460
    },
    {
      "epoch": 1.335948065999459,
      "grad_norm": 0.7763209939002991,
      "learning_rate": 0.00011093073593073593,
      "loss": 1.8131,
      "step": 2470
    },
    {
      "epoch": 1.3413578577224776,
      "grad_norm": 0.5684813857078552,
      "learning_rate": 0.00011056998556998557,
      "loss": 1.858,
      "step": 2480
    },
    {
      "epoch": 1.3467676494454963,
      "grad_norm": 0.7212191224098206,
      "learning_rate": 0.0001102092352092352,
      "loss": 1.9711,
      "step": 2490
    },
    {
      "epoch": 1.352177441168515,
      "grad_norm": 0.6510563492774963,
      "learning_rate": 0.00010984848484848484,
      "loss": 1.7465,
      "step": 2500
    },
    {
      "epoch": 1.3575872328915337,
      "grad_norm": 0.5989896059036255,
      "learning_rate": 0.00010948773448773448,
      "loss": 1.6315,
      "step": 2510
    },
    {
      "epoch": 1.3629970246145524,
      "grad_norm": 0.5490729808807373,
      "learning_rate": 0.00010912698412698413,
      "loss": 1.818,
      "step": 2520
    },
    {
      "epoch": 1.368406816337571,
      "grad_norm": 0.6094787120819092,
      "learning_rate": 0.00010876623376623376,
      "loss": 1.7855,
      "step": 2530
    },
    {
      "epoch": 1.3738166080605896,
      "grad_norm": 0.5328908562660217,
      "learning_rate": 0.00010840548340548342,
      "loss": 1.7254,
      "step": 2540
    },
    {
      "epoch": 1.3792263997836083,
      "grad_norm": 0.7710531949996948,
      "learning_rate": 0.00010804473304473307,
      "loss": 1.7833,
      "step": 2550
    },
    {
      "epoch": 1.384636191506627,
      "grad_norm": 0.5582380890846252,
      "learning_rate": 0.0001076839826839827,
      "loss": 1.8529,
      "step": 2560
    },
    {
      "epoch": 1.3900459832296457,
      "grad_norm": 0.8822859525680542,
      "learning_rate": 0.00010732323232323234,
      "loss": 1.7939,
      "step": 2570
    },
    {
      "epoch": 1.3954557749526644,
      "grad_norm": 0.6184915900230408,
      "learning_rate": 0.00010696248196248198,
      "loss": 1.7734,
      "step": 2580
    },
    {
      "epoch": 1.4008655666756828,
      "grad_norm": 0.5608492493629456,
      "learning_rate": 0.00010660173160173161,
      "loss": 1.7491,
      "step": 2590
    },
    {
      "epoch": 1.4062753583987018,
      "grad_norm": 0.5640730261802673,
      "learning_rate": 0.00010624098124098125,
      "loss": 1.8128,
      "step": 2600
    },
    {
      "epoch": 1.4116851501217202,
      "grad_norm": 0.6378976106643677,
      "learning_rate": 0.00010588023088023089,
      "loss": 1.7751,
      "step": 2610
    },
    {
      "epoch": 1.417094941844739,
      "grad_norm": 0.6328241229057312,
      "learning_rate": 0.00010551948051948053,
      "loss": 1.736,
      "step": 2620
    },
    {
      "epoch": 1.4225047335677576,
      "grad_norm": 0.6812890768051147,
      "learning_rate": 0.00010515873015873016,
      "loss": 1.8422,
      "step": 2630
    },
    {
      "epoch": 1.4279145252907763,
      "grad_norm": 0.5100430846214294,
      "learning_rate": 0.0001047979797979798,
      "loss": 1.8522,
      "step": 2640
    },
    {
      "epoch": 1.433324317013795,
      "grad_norm": 0.691237211227417,
      "learning_rate": 0.00010443722943722945,
      "loss": 1.8587,
      "step": 2650
    },
    {
      "epoch": 1.4387341087368135,
      "grad_norm": 0.5038658380508423,
      "learning_rate": 0.00010407647907647909,
      "loss": 1.8409,
      "step": 2660
    },
    {
      "epoch": 1.4441439004598324,
      "grad_norm": 0.6618639230728149,
      "learning_rate": 0.00010371572871572872,
      "loss": 1.8407,
      "step": 2670
    },
    {
      "epoch": 1.449553692182851,
      "grad_norm": 0.5997622609138489,
      "learning_rate": 0.00010335497835497836,
      "loss": 1.7418,
      "step": 2680
    },
    {
      "epoch": 1.4549634839058696,
      "grad_norm": 0.495418518781662,
      "learning_rate": 0.000102994227994228,
      "loss": 1.78,
      "step": 2690
    },
    {
      "epoch": 1.4603732756288883,
      "grad_norm": 0.7518081068992615,
      "learning_rate": 0.00010263347763347764,
      "loss": 1.8881,
      "step": 2700
    },
    {
      "epoch": 1.465783067351907,
      "grad_norm": 0.600385844707489,
      "learning_rate": 0.00010227272727272727,
      "loss": 1.7336,
      "step": 2710
    },
    {
      "epoch": 1.4711928590749257,
      "grad_norm": 0.554000973701477,
      "learning_rate": 0.00010191197691197691,
      "loss": 1.8022,
      "step": 2720
    },
    {
      "epoch": 1.4766026507979442,
      "grad_norm": 0.7735956311225891,
      "learning_rate": 0.00010155122655122655,
      "loss": 1.7599,
      "step": 2730
    },
    {
      "epoch": 1.4820124425209629,
      "grad_norm": 0.587806761264801,
      "learning_rate": 0.0001011904761904762,
      "loss": 1.7545,
      "step": 2740
    },
    {
      "epoch": 1.4874222342439816,
      "grad_norm": 0.647062361240387,
      "learning_rate": 0.00010082972582972582,
      "loss": 1.7417,
      "step": 2750
    },
    {
      "epoch": 1.4928320259670003,
      "grad_norm": 0.6140472292900085,
      "learning_rate": 0.00010046897546897547,
      "loss": 1.8717,
      "step": 2760
    },
    {
      "epoch": 1.498241817690019,
      "grad_norm": 0.5784351825714111,
      "learning_rate": 0.00010010822510822511,
      "loss": 1.8528,
      "step": 2770
    },
    {
      "epoch": 1.5036516094130374,
      "grad_norm": 0.939140796661377,
      "learning_rate": 9.974747474747475e-05,
      "loss": 1.8616,
      "step": 2780
    },
    {
      "epoch": 1.5090614011360564,
      "grad_norm": 0.561713695526123,
      "learning_rate": 9.938672438672439e-05,
      "loss": 1.8314,
      "step": 2790
    },
    {
      "epoch": 1.5144711928590748,
      "grad_norm": 0.6377916932106018,
      "learning_rate": 9.902597402597403e-05,
      "loss": 1.6808,
      "step": 2800
    },
    {
      "epoch": 1.5198809845820938,
      "grad_norm": 0.5014219880104065,
      "learning_rate": 9.866522366522368e-05,
      "loss": 1.8483,
      "step": 2810
    },
    {
      "epoch": 1.5252907763051122,
      "grad_norm": 0.6416189074516296,
      "learning_rate": 9.83044733044733e-05,
      "loss": 1.7344,
      "step": 2820
    },
    {
      "epoch": 1.530700568028131,
      "grad_norm": 0.527767539024353,
      "learning_rate": 9.794372294372295e-05,
      "loss": 1.7351,
      "step": 2830
    },
    {
      "epoch": 1.5361103597511496,
      "grad_norm": 0.4503432810306549,
      "learning_rate": 9.758297258297259e-05,
      "loss": 1.7654,
      "step": 2840
    },
    {
      "epoch": 1.541520151474168,
      "grad_norm": 0.8823242783546448,
      "learning_rate": 9.722222222222223e-05,
      "loss": 1.7906,
      "step": 2850
    },
    {
      "epoch": 1.546929943197187,
      "grad_norm": 0.8000040650367737,
      "learning_rate": 9.686147186147186e-05,
      "loss": 1.8563,
      "step": 2860
    },
    {
      "epoch": 1.5523397349202055,
      "grad_norm": 0.8218516707420349,
      "learning_rate": 9.65007215007215e-05,
      "loss": 1.7826,
      "step": 2870
    },
    {
      "epoch": 1.5577495266432242,
      "grad_norm": 0.5908588767051697,
      "learning_rate": 9.613997113997114e-05,
      "loss": 1.7484,
      "step": 2880
    },
    {
      "epoch": 1.563159318366243,
      "grad_norm": 0.5984391570091248,
      "learning_rate": 9.577922077922078e-05,
      "loss": 1.6992,
      "step": 2890
    },
    {
      "epoch": 1.5685691100892616,
      "grad_norm": 0.5694805979728699,
      "learning_rate": 9.541847041847041e-05,
      "loss": 1.8712,
      "step": 2900
    },
    {
      "epoch": 1.5739789018122803,
      "grad_norm": 0.7169468402862549,
      "learning_rate": 9.505772005772007e-05,
      "loss": 1.969,
      "step": 2910
    },
    {
      "epoch": 1.5793886935352988,
      "grad_norm": 0.6731020212173462,
      "learning_rate": 9.469696969696971e-05,
      "loss": 1.8057,
      "step": 2920
    },
    {
      "epoch": 1.5847984852583177,
      "grad_norm": 0.6578049659729004,
      "learning_rate": 9.433621933621934e-05,
      "loss": 1.8464,
      "step": 2930
    },
    {
      "epoch": 1.5902082769813362,
      "grad_norm": 0.6680458188056946,
      "learning_rate": 9.397546897546898e-05,
      "loss": 1.7579,
      "step": 2940
    },
    {
      "epoch": 1.5956180687043549,
      "grad_norm": 0.6551367044448853,
      "learning_rate": 9.361471861471862e-05,
      "loss": 1.7882,
      "step": 2950
    },
    {
      "epoch": 1.6010278604273736,
      "grad_norm": 0.8830645084381104,
      "learning_rate": 9.325396825396826e-05,
      "loss": 1.8037,
      "step": 2960
    },
    {
      "epoch": 1.6064376521503922,
      "grad_norm": 0.6841064095497131,
      "learning_rate": 9.289321789321789e-05,
      "loss": 1.809,
      "step": 2970
    },
    {
      "epoch": 1.611847443873411,
      "grad_norm": 0.7406896352767944,
      "learning_rate": 9.253246753246754e-05,
      "loss": 1.7145,
      "step": 2980
    },
    {
      "epoch": 1.6172572355964294,
      "grad_norm": 0.7577963471412659,
      "learning_rate": 9.217171717171718e-05,
      "loss": 1.7533,
      "step": 2990
    },
    {
      "epoch": 1.6226670273194483,
      "grad_norm": 0.5707703232765198,
      "learning_rate": 9.181096681096682e-05,
      "loss": 1.848,
      "step": 3000
    },
    {
      "epoch": 1.6280768190424668,
      "grad_norm": 0.7727959156036377,
      "learning_rate": 9.145021645021645e-05,
      "loss": 1.837,
      "step": 3010
    },
    {
      "epoch": 1.6334866107654855,
      "grad_norm": 0.9029533863067627,
      "learning_rate": 9.108946608946609e-05,
      "loss": 1.8699,
      "step": 3020
    },
    {
      "epoch": 1.6388964024885042,
      "grad_norm": 0.686570942401886,
      "learning_rate": 9.072871572871573e-05,
      "loss": 1.7136,
      "step": 3030
    },
    {
      "epoch": 1.644306194211523,
      "grad_norm": 0.6069352030754089,
      "learning_rate": 9.036796536796537e-05,
      "loss": 1.8569,
      "step": 3040
    },
    {
      "epoch": 1.6497159859345416,
      "grad_norm": 0.658044695854187,
      "learning_rate": 9.0007215007215e-05,
      "loss": 1.7424,
      "step": 3050
    },
    {
      "epoch": 1.65512577765756,
      "grad_norm": 0.6367936134338379,
      "learning_rate": 8.964646464646466e-05,
      "loss": 1.7717,
      "step": 3060
    },
    {
      "epoch": 1.660535569380579,
      "grad_norm": 0.7849181294441223,
      "learning_rate": 8.92857142857143e-05,
      "loss": 1.8601,
      "step": 3070
    },
    {
      "epoch": 1.6659453611035975,
      "grad_norm": 0.5776723623275757,
      "learning_rate": 8.892496392496393e-05,
      "loss": 1.8814,
      "step": 3080
    },
    {
      "epoch": 1.6713551528266162,
      "grad_norm": 0.560427188873291,
      "learning_rate": 8.856421356421357e-05,
      "loss": 1.7318,
      "step": 3090
    },
    {
      "epoch": 1.6767649445496349,
      "grad_norm": 0.6509298086166382,
      "learning_rate": 8.820346320346321e-05,
      "loss": 1.7832,
      "step": 3100
    },
    {
      "epoch": 1.6821747362726533,
      "grad_norm": 0.7135399580001831,
      "learning_rate": 8.784271284271285e-05,
      "loss": 1.9348,
      "step": 3110
    },
    {
      "epoch": 1.6875845279956723,
      "grad_norm": 0.5975245237350464,
      "learning_rate": 8.748196248196248e-05,
      "loss": 1.8347,
      "step": 3120
    },
    {
      "epoch": 1.6929943197186907,
      "grad_norm": 0.6246079206466675,
      "learning_rate": 8.712121212121212e-05,
      "loss": 1.8552,
      "step": 3130
    },
    {
      "epoch": 1.6984041114417094,
      "grad_norm": 0.6166945099830627,
      "learning_rate": 8.676046176046177e-05,
      "loss": 1.7327,
      "step": 3140
    },
    {
      "epoch": 1.7038139031647281,
      "grad_norm": 0.5571082234382629,
      "learning_rate": 8.639971139971141e-05,
      "loss": 1.7909,
      "step": 3150
    },
    {
      "epoch": 1.7092236948877468,
      "grad_norm": 0.7227116823196411,
      "learning_rate": 8.603896103896104e-05,
      "loss": 1.7338,
      "step": 3160
    },
    {
      "epoch": 1.7146334866107655,
      "grad_norm": 0.5448179841041565,
      "learning_rate": 8.567821067821068e-05,
      "loss": 1.8182,
      "step": 3170
    },
    {
      "epoch": 1.720043278333784,
      "grad_norm": 0.5541513562202454,
      "learning_rate": 8.531746031746032e-05,
      "loss": 1.7549,
      "step": 3180
    },
    {
      "epoch": 1.725453070056803,
      "grad_norm": 0.6774485111236572,
      "learning_rate": 8.495670995670996e-05,
      "loss": 1.8686,
      "step": 3190
    },
    {
      "epoch": 1.7308628617798214,
      "grad_norm": 0.5630931854248047,
      "learning_rate": 8.459595959595959e-05,
      "loss": 1.7225,
      "step": 3200
    },
    {
      "epoch": 1.73627265350284,
      "grad_norm": 0.6754251718521118,
      "learning_rate": 8.423520923520925e-05,
      "loss": 1.7643,
      "step": 3210
    },
    {
      "epoch": 1.7416824452258588,
      "grad_norm": 0.47501692175865173,
      "learning_rate": 8.387445887445889e-05,
      "loss": 1.8024,
      "step": 3220
    },
    {
      "epoch": 1.7470922369488775,
      "grad_norm": 0.5571823120117188,
      "learning_rate": 8.351370851370852e-05,
      "loss": 1.9131,
      "step": 3230
    },
    {
      "epoch": 1.7525020286718962,
      "grad_norm": 0.5791082978248596,
      "learning_rate": 8.315295815295816e-05,
      "loss": 1.892,
      "step": 3240
    },
    {
      "epoch": 1.7579118203949147,
      "grad_norm": 0.5932286381721497,
      "learning_rate": 8.27922077922078e-05,
      "loss": 1.8743,
      "step": 3250
    },
    {
      "epoch": 1.7633216121179336,
      "grad_norm": 0.732574999332428,
      "learning_rate": 8.243145743145744e-05,
      "loss": 1.7996,
      "step": 3260
    },
    {
      "epoch": 1.768731403840952,
      "grad_norm": 0.7538799047470093,
      "learning_rate": 8.207070707070707e-05,
      "loss": 1.7749,
      "step": 3270
    },
    {
      "epoch": 1.7741411955639708,
      "grad_norm": 0.6947706937789917,
      "learning_rate": 8.170995670995671e-05,
      "loss": 1.8034,
      "step": 3280
    },
    {
      "epoch": 1.7795509872869895,
      "grad_norm": 0.5295731425285339,
      "learning_rate": 8.134920634920635e-05,
      "loss": 1.7385,
      "step": 3290
    },
    {
      "epoch": 1.7849607790100082,
      "grad_norm": 0.5981854796409607,
      "learning_rate": 8.0988455988456e-05,
      "loss": 1.8233,
      "step": 3300
    },
    {
      "epoch": 1.7903705707330269,
      "grad_norm": 0.685311496257782,
      "learning_rate": 8.062770562770562e-05,
      "loss": 1.8406,
      "step": 3310
    },
    {
      "epoch": 1.7957803624560453,
      "grad_norm": 0.7737095355987549,
      "learning_rate": 8.026695526695527e-05,
      "loss": 1.96,
      "step": 3320
    },
    {
      "epoch": 1.8011901541790643,
      "grad_norm": 0.8853969573974609,
      "learning_rate": 7.990620490620491e-05,
      "loss": 1.7718,
      "step": 3330
    },
    {
      "epoch": 1.8065999459020827,
      "grad_norm": 0.6886159181594849,
      "learning_rate": 7.954545454545455e-05,
      "loss": 1.9631,
      "step": 3340
    },
    {
      "epoch": 1.8120097376251014,
      "grad_norm": 0.5824671983718872,
      "learning_rate": 7.918470418470418e-05,
      "loss": 1.9432,
      "step": 3350
    },
    {
      "epoch": 1.8174195293481201,
      "grad_norm": 0.6573203802108765,
      "learning_rate": 7.882395382395382e-05,
      "loss": 1.7902,
      "step": 3360
    },
    {
      "epoch": 1.8228293210711386,
      "grad_norm": 0.6482689380645752,
      "learning_rate": 7.846320346320348e-05,
      "loss": 1.8927,
      "step": 3370
    },
    {
      "epoch": 1.8282391127941575,
      "grad_norm": 0.6681481599807739,
      "learning_rate": 7.81024531024531e-05,
      "loss": 1.847,
      "step": 3380
    },
    {
      "epoch": 1.833648904517176,
      "grad_norm": 0.6683822274208069,
      "learning_rate": 7.774170274170275e-05,
      "loss": 1.7883,
      "step": 3390
    },
    {
      "epoch": 1.839058696240195,
      "grad_norm": 0.6146317720413208,
      "learning_rate": 7.738095238095239e-05,
      "loss": 1.7835,
      "step": 3400
    },
    {
      "epoch": 1.8444684879632134,
      "grad_norm": 0.5073976516723633,
      "learning_rate": 7.702020202020203e-05,
      "loss": 1.8328,
      "step": 3410
    },
    {
      "epoch": 1.849878279686232,
      "grad_norm": 0.6550754308700562,
      "learning_rate": 7.665945165945166e-05,
      "loss": 1.9235,
      "step": 3420
    },
    {
      "epoch": 1.8552880714092508,
      "grad_norm": 0.5955948829650879,
      "learning_rate": 7.62987012987013e-05,
      "loss": 1.7205,
      "step": 3430
    },
    {
      "epoch": 1.8606978631322693,
      "grad_norm": 0.756870448589325,
      "learning_rate": 7.593795093795094e-05,
      "loss": 1.8814,
      "step": 3440
    },
    {
      "epoch": 1.8661076548552882,
      "grad_norm": 0.6271041035652161,
      "learning_rate": 7.557720057720059e-05,
      "loss": 1.7525,
      "step": 3450
    },
    {
      "epoch": 1.8715174465783067,
      "grad_norm": 0.772280752658844,
      "learning_rate": 7.521645021645021e-05,
      "loss": 1.85,
      "step": 3460
    },
    {
      "epoch": 1.8769272383013254,
      "grad_norm": 0.7514734268188477,
      "learning_rate": 7.485569985569986e-05,
      "loss": 1.9103,
      "step": 3470
    },
    {
      "epoch": 1.882337030024344,
      "grad_norm": 1.0703790187835693,
      "learning_rate": 7.44949494949495e-05,
      "loss": 1.7489,
      "step": 3480
    },
    {
      "epoch": 1.8877468217473627,
      "grad_norm": 0.6420645117759705,
      "learning_rate": 7.413419913419914e-05,
      "loss": 1.7915,
      "step": 3490
    },
    {
      "epoch": 1.8931566134703814,
      "grad_norm": 0.5953499674797058,
      "learning_rate": 7.377344877344877e-05,
      "loss": 1.9287,
      "step": 3500
    },
    {
      "epoch": 1.8985664051934,
      "grad_norm": 0.6354008316993713,
      "learning_rate": 7.341269841269841e-05,
      "loss": 1.8396,
      "step": 3510
    },
    {
      "epoch": 1.9039761969164188,
      "grad_norm": 0.721768319606781,
      "learning_rate": 7.305194805194807e-05,
      "loss": 1.8521,
      "step": 3520
    },
    {
      "epoch": 1.9093859886394373,
      "grad_norm": 1.023544430732727,
      "learning_rate": 7.26911976911977e-05,
      "loss": 1.8166,
      "step": 3530
    },
    {
      "epoch": 1.914795780362456,
      "grad_norm": 0.5791183114051819,
      "learning_rate": 7.233044733044734e-05,
      "loss": 1.8748,
      "step": 3540
    },
    {
      "epoch": 1.9202055720854747,
      "grad_norm": 0.7738423943519592,
      "learning_rate": 7.196969696969698e-05,
      "loss": 1.9068,
      "step": 3550
    },
    {
      "epoch": 1.9256153638084934,
      "grad_norm": 0.639610767364502,
      "learning_rate": 7.160894660894662e-05,
      "loss": 1.8105,
      "step": 3560
    },
    {
      "epoch": 1.931025155531512,
      "grad_norm": 0.47337818145751953,
      "learning_rate": 7.124819624819625e-05,
      "loss": 1.7744,
      "step": 3570
    },
    {
      "epoch": 1.9364349472545306,
      "grad_norm": 0.7621448636054993,
      "learning_rate": 7.088744588744589e-05,
      "loss": 1.7798,
      "step": 3580
    },
    {
      "epoch": 1.9418447389775495,
      "grad_norm": 0.614712119102478,
      "learning_rate": 7.052669552669553e-05,
      "loss": 1.761,
      "step": 3590
    },
    {
      "epoch": 1.947254530700568,
      "grad_norm": 0.5357441902160645,
      "learning_rate": 7.016594516594517e-05,
      "loss": 1.8719,
      "step": 3600
    },
    {
      "epoch": 1.9526643224235867,
      "grad_norm": 0.6692637801170349,
      "learning_rate": 6.98051948051948e-05,
      "loss": 1.7763,
      "step": 3610
    },
    {
      "epoch": 1.9580741141466054,
      "grad_norm": 0.49973616003990173,
      "learning_rate": 6.944444444444444e-05,
      "loss": 1.7547,
      "step": 3620
    },
    {
      "epoch": 1.963483905869624,
      "grad_norm": 0.6778360605239868,
      "learning_rate": 6.908369408369409e-05,
      "loss": 1.8783,
      "step": 3630
    },
    {
      "epoch": 1.9688936975926428,
      "grad_norm": 0.6741539239883423,
      "learning_rate": 6.872294372294373e-05,
      "loss": 1.661,
      "step": 3640
    },
    {
      "epoch": 1.9743034893156612,
      "grad_norm": 0.6905511021614075,
      "learning_rate": 6.836219336219336e-05,
      "loss": 1.9004,
      "step": 3650
    },
    {
      "epoch": 1.9797132810386802,
      "grad_norm": 0.6132948398590088,
      "learning_rate": 6.8001443001443e-05,
      "loss": 1.9334,
      "step": 3660
    },
    {
      "epoch": 1.9851230727616986,
      "grad_norm": 0.6923881769180298,
      "learning_rate": 6.764069264069265e-05,
      "loss": 1.6967,
      "step": 3670
    },
    {
      "epoch": 1.9905328644847173,
      "grad_norm": 0.610485315322876,
      "learning_rate": 6.727994227994228e-05,
      "loss": 1.7567,
      "step": 3680
    },
    {
      "epoch": 1.995942656207736,
      "grad_norm": 0.6126591563224792,
      "learning_rate": 6.691919191919192e-05,
      "loss": 1.7971,
      "step": 3690
    },
    {
      "epoch": 2.001081958344604,
      "grad_norm": 0.8340866565704346,
      "learning_rate": 6.655844155844157e-05,
      "loss": 1.8718,
      "step": 3700
    },
    {
      "epoch": 2.0064917500676223,
      "grad_norm": 0.6052669286727905,
      "learning_rate": 6.619769119769121e-05,
      "loss": 1.7665,
      "step": 3710
    },
    {
      "epoch": 2.011901541790641,
      "grad_norm": 0.7516818642616272,
      "learning_rate": 6.583694083694084e-05,
      "loss": 1.7786,
      "step": 3720
    },
    {
      "epoch": 2.0173113335136597,
      "grad_norm": 0.5235362648963928,
      "learning_rate": 6.547619047619048e-05,
      "loss": 1.8243,
      "step": 3730
    },
    {
      "epoch": 2.0227211252366786,
      "grad_norm": 0.6377628445625305,
      "learning_rate": 6.511544011544012e-05,
      "loss": 1.774,
      "step": 3740
    },
    {
      "epoch": 2.028130916959697,
      "grad_norm": 0.6021323800086975,
      "learning_rate": 6.475468975468976e-05,
      "loss": 1.7776,
      "step": 3750
    },
    {
      "epoch": 2.0335407086827155,
      "grad_norm": 0.5629180669784546,
      "learning_rate": 6.439393939393939e-05,
      "loss": 1.7179,
      "step": 3760
    },
    {
      "epoch": 2.0389505004057344,
      "grad_norm": 0.5057544112205505,
      "learning_rate": 6.403318903318903e-05,
      "loss": 1.9531,
      "step": 3770
    },
    {
      "epoch": 2.044360292128753,
      "grad_norm": 0.5570243000984192,
      "learning_rate": 6.367243867243868e-05,
      "loss": 1.804,
      "step": 3780
    },
    {
      "epoch": 2.049770083851772,
      "grad_norm": 0.6440011262893677,
      "learning_rate": 6.331168831168832e-05,
      "loss": 1.9395,
      "step": 3790
    },
    {
      "epoch": 2.0551798755747903,
      "grad_norm": 0.6192986369132996,
      "learning_rate": 6.295093795093795e-05,
      "loss": 1.8536,
      "step": 3800
    },
    {
      "epoch": 2.0605896672978092,
      "grad_norm": 0.605377197265625,
      "learning_rate": 6.259018759018759e-05,
      "loss": 1.8109,
      "step": 3810
    },
    {
      "epoch": 2.0659994590208277,
      "grad_norm": 0.7739197015762329,
      "learning_rate": 6.222943722943724e-05,
      "loss": 1.8667,
      "step": 3820
    },
    {
      "epoch": 2.071409250743846,
      "grad_norm": 0.8820847272872925,
      "learning_rate": 6.186868686868687e-05,
      "loss": 1.8223,
      "step": 3830
    },
    {
      "epoch": 2.076819042466865,
      "grad_norm": 0.5591381788253784,
      "learning_rate": 6.150793650793651e-05,
      "loss": 1.7141,
      "step": 3840
    },
    {
      "epoch": 2.0822288341898836,
      "grad_norm": 0.5210393071174622,
      "learning_rate": 6.114718614718616e-05,
      "loss": 1.7628,
      "step": 3850
    },
    {
      "epoch": 2.0876386259129025,
      "grad_norm": 0.6508904695510864,
      "learning_rate": 6.078643578643579e-05,
      "loss": 1.7715,
      "step": 3860
    },
    {
      "epoch": 2.093048417635921,
      "grad_norm": 0.831100344657898,
      "learning_rate": 6.042568542568543e-05,
      "loss": 1.8207,
      "step": 3870
    },
    {
      "epoch": 2.09845820935894,
      "grad_norm": 0.48767173290252686,
      "learning_rate": 6.006493506493507e-05,
      "loss": 1.7551,
      "step": 3880
    },
    {
      "epoch": 2.1038680010819584,
      "grad_norm": 0.8048025965690613,
      "learning_rate": 5.970418470418471e-05,
      "loss": 1.7291,
      "step": 3890
    },
    {
      "epoch": 2.109277792804977,
      "grad_norm": 0.6249663233757019,
      "learning_rate": 5.9343434343434345e-05,
      "loss": 1.7776,
      "step": 3900
    },
    {
      "epoch": 2.1146875845279958,
      "grad_norm": 0.7308489084243774,
      "learning_rate": 5.898268398268399e-05,
      "loss": 1.782,
      "step": 3910
    },
    {
      "epoch": 2.1200973762510142,
      "grad_norm": 0.6730958223342896,
      "learning_rate": 5.862193362193362e-05,
      "loss": 1.8361,
      "step": 3920
    },
    {
      "epoch": 2.125507167974033,
      "grad_norm": 0.6117950677871704,
      "learning_rate": 5.8261183261183264e-05,
      "loss": 1.7814,
      "step": 3930
    },
    {
      "epoch": 2.1309169596970516,
      "grad_norm": 0.6009824275970459,
      "learning_rate": 5.79004329004329e-05,
      "loss": 1.6991,
      "step": 3940
    },
    {
      "epoch": 2.1363267514200706,
      "grad_norm": 0.6003170013427734,
      "learning_rate": 5.753968253968254e-05,
      "loss": 1.8834,
      "step": 3950
    },
    {
      "epoch": 2.141736543143089,
      "grad_norm": 0.7710368633270264,
      "learning_rate": 5.7178932178932176e-05,
      "loss": 1.7126,
      "step": 3960
    },
    {
      "epoch": 2.1471463348661075,
      "grad_norm": 0.5985333323478699,
      "learning_rate": 5.6818181818181825e-05,
      "loss": 1.6981,
      "step": 3970
    },
    {
      "epoch": 2.1525561265891264,
      "grad_norm": 0.5864068865776062,
      "learning_rate": 5.645743145743147e-05,
      "loss": 1.8205,
      "step": 3980
    },
    {
      "epoch": 2.157965918312145,
      "grad_norm": 0.6955819725990295,
      "learning_rate": 5.60966810966811e-05,
      "loss": 1.8047,
      "step": 3990
    },
    {
      "epoch": 2.163375710035164,
      "grad_norm": 0.85777747631073,
      "learning_rate": 5.5735930735930744e-05,
      "loss": 1.7348,
      "step": 4000
    },
    {
      "epoch": 2.1687855017581823,
      "grad_norm": 0.7735285758972168,
      "learning_rate": 5.537518037518038e-05,
      "loss": 1.7159,
      "step": 4010
    },
    {
      "epoch": 2.1741952934812008,
      "grad_norm": 0.6233482956886292,
      "learning_rate": 5.501443001443002e-05,
      "loss": 1.7724,
      "step": 4020
    },
    {
      "epoch": 2.1796050852042197,
      "grad_norm": 0.7254752516746521,
      "learning_rate": 5.4653679653679656e-05,
      "loss": 1.8163,
      "step": 4030
    },
    {
      "epoch": 2.185014876927238,
      "grad_norm": 0.6261424422264099,
      "learning_rate": 5.42929292929293e-05,
      "loss": 1.8704,
      "step": 4040
    },
    {
      "epoch": 2.190424668650257,
      "grad_norm": 0.6496713757514954,
      "learning_rate": 5.3932178932178933e-05,
      "loss": 1.8001,
      "step": 4050
    },
    {
      "epoch": 2.1958344603732756,
      "grad_norm": 0.5798624753952026,
      "learning_rate": 5.3571428571428575e-05,
      "loss": 1.7469,
      "step": 4060
    },
    {
      "epoch": 2.2012442520962945,
      "grad_norm": 0.5256512761116028,
      "learning_rate": 5.321067821067821e-05,
      "loss": 1.7828,
      "step": 4070
    },
    {
      "epoch": 2.206654043819313,
      "grad_norm": 0.7992753982543945,
      "learning_rate": 5.284992784992785e-05,
      "loss": 1.8946,
      "step": 4080
    },
    {
      "epoch": 2.2120638355423314,
      "grad_norm": 0.6793275475502014,
      "learning_rate": 5.248917748917749e-05,
      "loss": 1.7887,
      "step": 4090
    },
    {
      "epoch": 2.2174736272653504,
      "grad_norm": 0.7913818359375,
      "learning_rate": 5.212842712842713e-05,
      "loss": 1.8589,
      "step": 4100
    },
    {
      "epoch": 2.222883418988369,
      "grad_norm": 0.6959556341171265,
      "learning_rate": 5.1767676767676765e-05,
      "loss": 1.8318,
      "step": 4110
    },
    {
      "epoch": 2.2282932107113878,
      "grad_norm": 0.6115196347236633,
      "learning_rate": 5.1406926406926414e-05,
      "loss": 1.8185,
      "step": 4120
    },
    {
      "epoch": 2.2337030024344062,
      "grad_norm": 0.6866345405578613,
      "learning_rate": 5.1046176046176056e-05,
      "loss": 1.9002,
      "step": 4130
    },
    {
      "epoch": 2.239112794157425,
      "grad_norm": 0.7472881078720093,
      "learning_rate": 5.068542568542569e-05,
      "loss": 1.7906,
      "step": 4140
    },
    {
      "epoch": 2.2445225858804436,
      "grad_norm": 0.6567680835723877,
      "learning_rate": 5.032467532467533e-05,
      "loss": 1.8331,
      "step": 4150
    },
    {
      "epoch": 2.249932377603462,
      "grad_norm": 0.6131584644317627,
      "learning_rate": 4.996392496392497e-05,
      "loss": 1.8975,
      "step": 4160
    },
    {
      "epoch": 2.255342169326481,
      "grad_norm": 0.5768475532531738,
      "learning_rate": 4.960317460317461e-05,
      "loss": 1.7413,
      "step": 4170
    },
    {
      "epoch": 2.2607519610494995,
      "grad_norm": 0.6162042617797852,
      "learning_rate": 4.9242424242424245e-05,
      "loss": 1.82,
      "step": 4180
    },
    {
      "epoch": 2.2661617527725184,
      "grad_norm": 0.6006482839584351,
      "learning_rate": 4.888167388167389e-05,
      "loss": 1.8512,
      "step": 4190
    },
    {
      "epoch": 2.271571544495537,
      "grad_norm": 0.6690912246704102,
      "learning_rate": 4.852092352092352e-05,
      "loss": 1.7194,
      "step": 4200
    },
    {
      "epoch": 2.2769813362185554,
      "grad_norm": 0.8103436827659607,
      "learning_rate": 4.8160173160173164e-05,
      "loss": 1.8798,
      "step": 4210
    },
    {
      "epoch": 2.2823911279415743,
      "grad_norm": 0.7372007966041565,
      "learning_rate": 4.77994227994228e-05,
      "loss": 1.8462,
      "step": 4220
    },
    {
      "epoch": 2.2878009196645928,
      "grad_norm": NaN,
      "learning_rate": 4.743867243867244e-05,
      "loss": 1.7145,
      "step": 4230
    },
    {
      "epoch": 2.2932107113876117,
      "grad_norm": 0.8913210034370422,
      "learning_rate": 4.711399711399712e-05,
      "loss": 1.8212,
      "step": 4240
    },
    {
      "epoch": 2.29862050311063,
      "grad_norm": 0.6523498892784119,
      "learning_rate": 4.675324675324675e-05,
      "loss": 1.6252,
      "step": 4250
    },
    {
      "epoch": 2.304030294833649,
      "grad_norm": 0.7713435292243958,
      "learning_rate": 4.6392496392496395e-05,
      "loss": 1.8112,
      "step": 4260
    },
    {
      "epoch": 2.3094400865566675,
      "grad_norm": 0.6967324614524841,
      "learning_rate": 4.603174603174603e-05,
      "loss": 1.8872,
      "step": 4270
    },
    {
      "epoch": 2.314849878279686,
      "grad_norm": 0.5376811027526855,
      "learning_rate": 4.567099567099568e-05,
      "loss": 1.9187,
      "step": 4280
    },
    {
      "epoch": 2.320259670002705,
      "grad_norm": 0.6632064580917358,
      "learning_rate": 4.5310245310245314e-05,
      "loss": 1.8199,
      "step": 4290
    },
    {
      "epoch": 2.3256694617257234,
      "grad_norm": 0.7898815274238586,
      "learning_rate": 4.494949494949495e-05,
      "loss": 1.7993,
      "step": 4300
    },
    {
      "epoch": 2.3310792534487423,
      "grad_norm": 0.650435209274292,
      "learning_rate": 4.458874458874459e-05,
      "loss": 1.6837,
      "step": 4310
    },
    {
      "epoch": 2.336489045171761,
      "grad_norm": 0.7922765612602234,
      "learning_rate": 4.4227994227994226e-05,
      "loss": 1.7794,
      "step": 4320
    },
    {
      "epoch": 2.3418988368947797,
      "grad_norm": 0.5828031301498413,
      "learning_rate": 4.386724386724387e-05,
      "loss": 1.7983,
      "step": 4330
    },
    {
      "epoch": 2.347308628617798,
      "grad_norm": 0.6279493570327759,
      "learning_rate": 4.3506493506493503e-05,
      "loss": 1.6813,
      "step": 4340
    },
    {
      "epoch": 2.3527184203408167,
      "grad_norm": 0.7413955926895142,
      "learning_rate": 4.314574314574315e-05,
      "loss": 1.7071,
      "step": 4350
    },
    {
      "epoch": 2.3581282120638356,
      "grad_norm": 0.6753249168395996,
      "learning_rate": 4.278499278499279e-05,
      "loss": 1.7581,
      "step": 4360
    },
    {
      "epoch": 2.363538003786854,
      "grad_norm": 0.8334604501724243,
      "learning_rate": 4.242424242424243e-05,
      "loss": 1.7809,
      "step": 4370
    },
    {
      "epoch": 2.368947795509873,
      "grad_norm": 0.742425262928009,
      "learning_rate": 4.2063492063492065e-05,
      "loss": 1.8831,
      "step": 4380
    },
    {
      "epoch": 2.3743575872328915,
      "grad_norm": 0.6401156783103943,
      "learning_rate": 4.1702741702741707e-05,
      "loss": 1.8608,
      "step": 4390
    },
    {
      "epoch": 2.3797673789559104,
      "grad_norm": 0.6427655816078186,
      "learning_rate": 4.134199134199134e-05,
      "loss": 1.9044,
      "step": 4400
    },
    {
      "epoch": 2.385177170678929,
      "grad_norm": 0.6656625866889954,
      "learning_rate": 4.0981240981240984e-05,
      "loss": 1.8224,
      "step": 4410
    },
    {
      "epoch": 2.3905869624019473,
      "grad_norm": 0.7228630185127258,
      "learning_rate": 4.062049062049062e-05,
      "loss": 1.8112,
      "step": 4420
    },
    {
      "epoch": 2.3959967541249663,
      "grad_norm": 0.7314060926437378,
      "learning_rate": 4.025974025974026e-05,
      "loss": 1.7979,
      "step": 4430
    },
    {
      "epoch": 2.4014065458479847,
      "grad_norm": 0.6166870594024658,
      "learning_rate": 3.98989898989899e-05,
      "loss": 1.7275,
      "step": 4440
    },
    {
      "epoch": 2.4068163375710037,
      "grad_norm": 0.9607617855072021,
      "learning_rate": 3.953823953823954e-05,
      "loss": 1.6347,
      "step": 4450
    },
    {
      "epoch": 2.412226129294022,
      "grad_norm": 0.709576427936554,
      "learning_rate": 3.917748917748918e-05,
      "loss": 1.7861,
      "step": 4460
    },
    {
      "epoch": 2.417635921017041,
      "grad_norm": 0.6295060515403748,
      "learning_rate": 3.8816738816738815e-05,
      "loss": 1.8564,
      "step": 4470
    },
    {
      "epoch": 2.4230457127400595,
      "grad_norm": 0.7323593497276306,
      "learning_rate": 3.845598845598846e-05,
      "loss": 1.8562,
      "step": 4480
    },
    {
      "epoch": 2.428455504463078,
      "grad_norm": 0.6023204326629639,
      "learning_rate": 3.809523809523809e-05,
      "loss": 1.7575,
      "step": 4490
    },
    {
      "epoch": 2.433865296186097,
      "grad_norm": 0.7642322182655334,
      "learning_rate": 3.773448773448774e-05,
      "loss": 1.739,
      "step": 4500
    },
    {
      "epoch": 2.4392750879091154,
      "grad_norm": 0.7144815921783447,
      "learning_rate": 3.7373737373737376e-05,
      "loss": 1.6316,
      "step": 4510
    },
    {
      "epoch": 2.4446848796321343,
      "grad_norm": 0.6409837007522583,
      "learning_rate": 3.701298701298702e-05,
      "loss": 1.7107,
      "step": 4520
    },
    {
      "epoch": 2.450094671355153,
      "grad_norm": 0.7017437219619751,
      "learning_rate": 3.665223665223665e-05,
      "loss": 1.7515,
      "step": 4530
    },
    {
      "epoch": 2.4555044630781717,
      "grad_norm": 0.6921395063400269,
      "learning_rate": 3.6291486291486295e-05,
      "loss": 1.7537,
      "step": 4540
    },
    {
      "epoch": 2.46091425480119,
      "grad_norm": 0.6924905180931091,
      "learning_rate": 3.593073593073593e-05,
      "loss": 1.7677,
      "step": 4550
    },
    {
      "epoch": 2.4663240465242087,
      "grad_norm": 0.5016525983810425,
      "learning_rate": 3.556998556998557e-05,
      "loss": 1.6652,
      "step": 4560
    },
    {
      "epoch": 2.4717338382472276,
      "grad_norm": 0.5852475166320801,
      "learning_rate": 3.520923520923521e-05,
      "loss": 1.8679,
      "step": 4570
    },
    {
      "epoch": 2.477143629970246,
      "grad_norm": 0.6921685338020325,
      "learning_rate": 3.484848484848485e-05,
      "loss": 1.7184,
      "step": 4580
    },
    {
      "epoch": 2.482553421693265,
      "grad_norm": 0.5761089324951172,
      "learning_rate": 3.448773448773449e-05,
      "loss": 1.8119,
      "step": 4590
    },
    {
      "epoch": 2.4879632134162835,
      "grad_norm": 0.7174144983291626,
      "learning_rate": 3.412698412698413e-05,
      "loss": 1.7742,
      "step": 4600
    },
    {
      "epoch": 2.4933730051393024,
      "grad_norm": 0.6484964489936829,
      "learning_rate": 3.376623376623377e-05,
      "loss": 1.8609,
      "step": 4610
    },
    {
      "epoch": 2.498782796862321,
      "grad_norm": 0.6031144857406616,
      "learning_rate": 3.3405483405483404e-05,
      "loss": 1.8014,
      "step": 4620
    },
    {
      "epoch": 2.5041925885853393,
      "grad_norm": 0.5977758169174194,
      "learning_rate": 3.3044733044733046e-05,
      "loss": 1.9104,
      "step": 4630
    },
    {
      "epoch": 2.5096023803083582,
      "grad_norm": 0.6604270935058594,
      "learning_rate": 3.268398268398268e-05,
      "loss": 1.8096,
      "step": 4640
    },
    {
      "epoch": 2.5150121720313767,
      "grad_norm": 0.6577782034873962,
      "learning_rate": 3.232323232323233e-05,
      "loss": 1.7152,
      "step": 4650
    },
    {
      "epoch": 2.520421963754395,
      "grad_norm": 0.7486181855201721,
      "learning_rate": 3.1962481962481965e-05,
      "loss": 1.7713,
      "step": 4660
    },
    {
      "epoch": 2.525831755477414,
      "grad_norm": 0.628166913986206,
      "learning_rate": 3.160173160173161e-05,
      "loss": 1.8618,
      "step": 4670
    },
    {
      "epoch": 2.531241547200433,
      "grad_norm": 0.7290562391281128,
      "learning_rate": 3.124098124098124e-05,
      "loss": 1.7203,
      "step": 4680
    },
    {
      "epoch": 2.5366513389234515,
      "grad_norm": 0.6974256038665771,
      "learning_rate": 3.0880230880230884e-05,
      "loss": 1.8361,
      "step": 4690
    },
    {
      "epoch": 2.54206113064647,
      "grad_norm": 0.5350431203842163,
      "learning_rate": 3.051948051948052e-05,
      "loss": 1.8051,
      "step": 4700
    },
    {
      "epoch": 2.547470922369489,
      "grad_norm": 0.7516698837280273,
      "learning_rate": 3.0158730158730158e-05,
      "loss": 1.8065,
      "step": 4710
    },
    {
      "epoch": 2.5528807140925074,
      "grad_norm": 0.7631978988647461,
      "learning_rate": 2.9797979797979796e-05,
      "loss": 1.7575,
      "step": 4720
    },
    {
      "epoch": 2.558290505815526,
      "grad_norm": 0.7650126814842224,
      "learning_rate": 2.943722943722944e-05,
      "loss": 1.845,
      "step": 4730
    },
    {
      "epoch": 2.563700297538545,
      "grad_norm": 0.6633756160736084,
      "learning_rate": 2.907647907647908e-05,
      "loss": 1.7359,
      "step": 4740
    },
    {
      "epoch": 2.5691100892615637,
      "grad_norm": 0.6118848919868469,
      "learning_rate": 2.871572871572872e-05,
      "loss": 1.656,
      "step": 4750
    },
    {
      "epoch": 2.574519880984582,
      "grad_norm": 0.7513632774353027,
      "learning_rate": 2.8354978354978357e-05,
      "loss": 1.9417,
      "step": 4760
    },
    {
      "epoch": 2.5799296727076007,
      "grad_norm": 0.6142874360084534,
      "learning_rate": 2.7994227994227996e-05,
      "loss": 1.923,
      "step": 4770
    },
    {
      "epoch": 2.5853394644306196,
      "grad_norm": 0.5753718614578247,
      "learning_rate": 2.7633477633477635e-05,
      "loss": 1.8203,
      "step": 4780
    },
    {
      "epoch": 2.590749256153638,
      "grad_norm": 0.7339443564414978,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 1.8137,
      "step": 4790
    },
    {
      "epoch": 2.5961590478766565,
      "grad_norm": 0.6574668288230896,
      "learning_rate": 2.691197691197691e-05,
      "loss": 1.7076,
      "step": 4800
    },
    {
      "epoch": 2.6015688395996754,
      "grad_norm": 0.8139709234237671,
      "learning_rate": 2.6551226551226554e-05,
      "loss": 1.7491,
      "step": 4810
    },
    {
      "epoch": 2.606978631322694,
      "grad_norm": 0.6254155039787292,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 1.6864,
      "step": 4820
    },
    {
      "epoch": 2.612388423045713,
      "grad_norm": 0.6273593902587891,
      "learning_rate": 2.582972582972583e-05,
      "loss": 1.7599,
      "step": 4830
    },
    {
      "epoch": 2.6177982147687313,
      "grad_norm": 0.5762760639190674,
      "learning_rate": 2.546897546897547e-05,
      "loss": 1.8116,
      "step": 4840
    },
    {
      "epoch": 2.6232080064917502,
      "grad_norm": 0.521897554397583,
      "learning_rate": 2.5108225108225108e-05,
      "loss": 1.8727,
      "step": 4850
    },
    {
      "epoch": 2.6286177982147687,
      "grad_norm": 0.715297281742096,
      "learning_rate": 2.474747474747475e-05,
      "loss": 1.7124,
      "step": 4860
    },
    {
      "epoch": 2.634027589937787,
      "grad_norm": 0.6992706060409546,
      "learning_rate": 2.438672438672439e-05,
      "loss": 1.806,
      "step": 4870
    },
    {
      "epoch": 2.639437381660806,
      "grad_norm": 0.8018097281455994,
      "learning_rate": 2.4025974025974027e-05,
      "loss": 1.7606,
      "step": 4880
    },
    {
      "epoch": 2.6448471733838246,
      "grad_norm": 0.653473973274231,
      "learning_rate": 2.3665223665223666e-05,
      "loss": 1.8124,
      "step": 4890
    },
    {
      "epoch": 2.6502569651068435,
      "grad_norm": 0.7274165153503418,
      "learning_rate": 2.3304473304473308e-05,
      "loss": 1.7948,
      "step": 4900
    },
    {
      "epoch": 2.655666756829862,
      "grad_norm": 0.5705764293670654,
      "learning_rate": 2.2943722943722946e-05,
      "loss": 1.6968,
      "step": 4910
    },
    {
      "epoch": 2.661076548552881,
      "grad_norm": 0.5700128078460693,
      "learning_rate": 2.2582972582972585e-05,
      "loss": 1.7899,
      "step": 4920
    },
    {
      "epoch": 2.6664863402758994,
      "grad_norm": 0.7787021994590759,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 1.7014,
      "step": 4930
    },
    {
      "epoch": 2.671896131998918,
      "grad_norm": 0.654510498046875,
      "learning_rate": 2.1861471861471862e-05,
      "loss": 1.7921,
      "step": 4940
    },
    {
      "epoch": 2.6773059237219368,
      "grad_norm": 0.6662071943283081,
      "learning_rate": 2.15007215007215e-05,
      "loss": 1.735,
      "step": 4950
    },
    {
      "epoch": 2.6827157154449552,
      "grad_norm": 0.5901893377304077,
      "learning_rate": 2.113997113997114e-05,
      "loss": 1.7696,
      "step": 4960
    },
    {
      "epoch": 2.688125507167974,
      "grad_norm": 0.7060858607292175,
      "learning_rate": 2.077922077922078e-05,
      "loss": 1.7782,
      "step": 4970
    },
    {
      "epoch": 2.6935352988909926,
      "grad_norm": 0.6351487636566162,
      "learning_rate": 2.041847041847042e-05,
      "loss": 1.7779,
      "step": 4980
    },
    {
      "epoch": 2.6989450906140116,
      "grad_norm": 0.8984031081199646,
      "learning_rate": 2.0057720057720058e-05,
      "loss": 1.7408,
      "step": 4990
    },
    {
      "epoch": 2.70435488233703,
      "grad_norm": 0.637194037437439,
      "learning_rate": 1.9696969696969697e-05,
      "loss": 1.6909,
      "step": 5000
    },
    {
      "epoch": 2.7097646740600485,
      "grad_norm": 0.5624322295188904,
      "learning_rate": 1.933621933621934e-05,
      "loss": 1.7062,
      "step": 5010
    },
    {
      "epoch": 2.7151744657830674,
      "grad_norm": 0.5504915118217468,
      "learning_rate": 1.8975468975468977e-05,
      "loss": 1.8619,
      "step": 5020
    },
    {
      "epoch": 2.720584257506086,
      "grad_norm": 0.48001226782798767,
      "learning_rate": 1.8614718614718616e-05,
      "loss": 1.9393,
      "step": 5030
    },
    {
      "epoch": 2.725994049229105,
      "grad_norm": 0.6653968691825867,
      "learning_rate": 1.8253968253968254e-05,
      "loss": 1.7501,
      "step": 5040
    },
    {
      "epoch": 2.7314038409521233,
      "grad_norm": 0.7113017439842224,
      "learning_rate": 1.7893217893217896e-05,
      "loss": 1.8103,
      "step": 5050
    },
    {
      "epoch": 2.736813632675142,
      "grad_norm": 0.6016008853912354,
      "learning_rate": 1.7532467532467535e-05,
      "loss": 1.6882,
      "step": 5060
    },
    {
      "epoch": 2.7422234243981607,
      "grad_norm": 0.6457864046096802,
      "learning_rate": 1.7171717171717173e-05,
      "loss": 1.8784,
      "step": 5070
    },
    {
      "epoch": 2.747633216121179,
      "grad_norm": 0.6702597737312317,
      "learning_rate": 1.6810966810966812e-05,
      "loss": 1.8413,
      "step": 5080
    },
    {
      "epoch": 2.753043007844198,
      "grad_norm": 0.6822138428688049,
      "learning_rate": 1.645021645021645e-05,
      "loss": 1.7528,
      "step": 5090
    },
    {
      "epoch": 2.7584527995672166,
      "grad_norm": 0.5909051299095154,
      "learning_rate": 1.608946608946609e-05,
      "loss": 1.7557,
      "step": 5100
    },
    {
      "epoch": 2.7638625912902355,
      "grad_norm": 0.8054590821266174,
      "learning_rate": 1.5728715728715728e-05,
      "loss": 1.6907,
      "step": 5110
    },
    {
      "epoch": 2.769272383013254,
      "grad_norm": 0.8266957998275757,
      "learning_rate": 1.5367965367965366e-05,
      "loss": 1.7617,
      "step": 5120
    },
    {
      "epoch": 2.774682174736273,
      "grad_norm": 0.5700217485427856,
      "learning_rate": 1.5007215007215008e-05,
      "loss": 1.7856,
      "step": 5130
    },
    {
      "epoch": 2.7800919664592914,
      "grad_norm": 0.7353121042251587,
      "learning_rate": 1.4646464646464647e-05,
      "loss": 1.8084,
      "step": 5140
    },
    {
      "epoch": 2.78550175818231,
      "grad_norm": 0.5920203924179077,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 1.6615,
      "step": 5150
    },
    {
      "epoch": 2.7909115499053287,
      "grad_norm": 0.595561146736145,
      "learning_rate": 1.3924963924963927e-05,
      "loss": 1.6755,
      "step": 5160
    },
    {
      "epoch": 2.7963213416283472,
      "grad_norm": 0.6091890335083008,
      "learning_rate": 1.3564213564213566e-05,
      "loss": 1.681,
      "step": 5170
    },
    {
      "epoch": 2.8017311333513657,
      "grad_norm": 0.8450314998626709,
      "learning_rate": 1.3203463203463205e-05,
      "loss": 1.8074,
      "step": 5180
    },
    {
      "epoch": 2.8071409250743846,
      "grad_norm": 0.7379552721977234,
      "learning_rate": 1.2842712842712843e-05,
      "loss": 1.6882,
      "step": 5190
    },
    {
      "epoch": 2.8125507167974035,
      "grad_norm": 0.9840508699417114,
      "learning_rate": 1.2481962481962482e-05,
      "loss": 1.7572,
      "step": 5200
    },
    {
      "epoch": 2.817960508520422,
      "grad_norm": 0.5295102596282959,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 1.6825,
      "step": 5210
    },
    {
      "epoch": 2.8233703002434405,
      "grad_norm": 0.6569264531135559,
      "learning_rate": 1.176046176046176e-05,
      "loss": 1.8131,
      "step": 5220
    },
    {
      "epoch": 2.8287800919664594,
      "grad_norm": 0.5898644328117371,
      "learning_rate": 1.13997113997114e-05,
      "loss": 1.6796,
      "step": 5230
    },
    {
      "epoch": 2.834189883689478,
      "grad_norm": 0.7016552686691284,
      "learning_rate": 1.103896103896104e-05,
      "loss": 1.7574,
      "step": 5240
    },
    {
      "epoch": 2.8395996754124964,
      "grad_norm": 0.606918454170227,
      "learning_rate": 1.067821067821068e-05,
      "loss": 1.7469,
      "step": 5250
    },
    {
      "epoch": 2.8450094671355153,
      "grad_norm": 0.5825967788696289,
      "learning_rate": 1.0317460317460318e-05,
      "loss": 1.8372,
      "step": 5260
    },
    {
      "epoch": 2.850419258858534,
      "grad_norm": 0.76353919506073,
      "learning_rate": 9.956709956709957e-06,
      "loss": 1.7939,
      "step": 5270
    },
    {
      "epoch": 2.8558290505815527,
      "grad_norm": 0.5589248538017273,
      "learning_rate": 9.595959595959595e-06,
      "loss": 1.7269,
      "step": 5280
    },
    {
      "epoch": 2.861238842304571,
      "grad_norm": 0.5226096510887146,
      "learning_rate": 9.235209235209236e-06,
      "loss": 1.6791,
      "step": 5290
    },
    {
      "epoch": 2.86664863402759,
      "grad_norm": 0.7090865969657898,
      "learning_rate": 8.874458874458876e-06,
      "loss": 1.7311,
      "step": 5300
    },
    {
      "epoch": 2.8720584257506085,
      "grad_norm": 0.595462441444397,
      "learning_rate": 8.513708513708514e-06,
      "loss": 1.7868,
      "step": 5310
    },
    {
      "epoch": 2.877468217473627,
      "grad_norm": 0.6938989162445068,
      "learning_rate": 8.152958152958155e-06,
      "loss": 1.8035,
      "step": 5320
    },
    {
      "epoch": 2.882878009196646,
      "grad_norm": 0.6519861817359924,
      "learning_rate": 7.792207792207792e-06,
      "loss": 1.8214,
      "step": 5330
    },
    {
      "epoch": 2.888287800919665,
      "grad_norm": 0.5750209093093872,
      "learning_rate": 7.431457431457433e-06,
      "loss": 1.6895,
      "step": 5340
    },
    {
      "epoch": 2.8936975926426833,
      "grad_norm": 0.7173929214477539,
      "learning_rate": 7.0707070707070704e-06,
      "loss": 1.744,
      "step": 5350
    },
    {
      "epoch": 2.899107384365702,
      "grad_norm": 0.5590521693229675,
      "learning_rate": 6.709956709956711e-06,
      "loss": 1.8086,
      "step": 5360
    },
    {
      "epoch": 2.9045171760887207,
      "grad_norm": 0.8863176107406616,
      "learning_rate": 6.349206349206349e-06,
      "loss": 1.8612,
      "step": 5370
    },
    {
      "epoch": 2.909926967811739,
      "grad_norm": 0.7052592635154724,
      "learning_rate": 5.988455988455989e-06,
      "loss": 1.8422,
      "step": 5380
    },
    {
      "epoch": 2.9153367595347577,
      "grad_norm": 0.696649432182312,
      "learning_rate": 5.627705627705628e-06,
      "loss": 1.8047,
      "step": 5390
    },
    {
      "epoch": 2.9207465512577766,
      "grad_norm": 0.6597380638122559,
      "learning_rate": 5.2669552669552675e-06,
      "loss": 1.8681,
      "step": 5400
    },
    {
      "epoch": 2.926156342980795,
      "grad_norm": 0.6304165124893188,
      "learning_rate": 4.906204906204907e-06,
      "loss": 1.8335,
      "step": 5410
    },
    {
      "epoch": 2.931566134703814,
      "grad_norm": 0.6937531232833862,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 1.7296,
      "step": 5420
    },
    {
      "epoch": 2.9369759264268325,
      "grad_norm": 0.5500021576881409,
      "learning_rate": 4.184704184704185e-06,
      "loss": 1.732,
      "step": 5430
    },
    {
      "epoch": 2.9423857181498514,
      "grad_norm": 0.8362081050872803,
      "learning_rate": 3.823953823953824e-06,
      "loss": 1.7106,
      "step": 5440
    },
    {
      "epoch": 2.94779550987287,
      "grad_norm": 0.5343526005744934,
      "learning_rate": 3.4632034632034634e-06,
      "loss": 1.7737,
      "step": 5450
    },
    {
      "epoch": 2.9532053015958883,
      "grad_norm": 0.7263351082801819,
      "learning_rate": 3.1024531024531023e-06,
      "loss": 1.8076,
      "step": 5460
    },
    {
      "epoch": 2.9586150933189073,
      "grad_norm": 0.5840311050415039,
      "learning_rate": 2.7417027417027418e-06,
      "loss": 1.8444,
      "step": 5470
    },
    {
      "epoch": 2.9640248850419257,
      "grad_norm": 0.6474947929382324,
      "learning_rate": 2.3809523809523808e-06,
      "loss": 1.6776,
      "step": 5480
    },
    {
      "epoch": 2.9694346767649447,
      "grad_norm": 0.7630051970481873,
      "learning_rate": 2.0202020202020206e-06,
      "loss": 1.6868,
      "step": 5490
    },
    {
      "epoch": 2.974844468487963,
      "grad_norm": 0.8876281380653381,
      "learning_rate": 1.6594516594516596e-06,
      "loss": 1.8207,
      "step": 5500
    },
    {
      "epoch": 2.980254260210982,
      "grad_norm": 0.8537352085113525,
      "learning_rate": 1.2987012987012988e-06,
      "loss": 1.8217,
      "step": 5510
    },
    {
      "epoch": 2.9856640519340005,
      "grad_norm": 0.7477399706840515,
      "learning_rate": 9.37950937950938e-07,
      "loss": 1.7549,
      "step": 5520
    },
    {
      "epoch": 2.991073843657019,
      "grad_norm": 0.6350899934768677,
      "learning_rate": 5.772005772005772e-07,
      "loss": 1.6547,
      "step": 5530
    },
    {
      "epoch": 2.996483635380038,
      "grad_norm": 0.7381577491760254,
      "learning_rate": 2.1645021645021646e-07,
      "loss": 1.8841,
      "step": 5540
    }
  ],
  "logging_steps": 10,
  "max_steps": 5544,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4123309666087731e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
